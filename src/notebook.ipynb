{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "# import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "RAW_DATA_FILE = \"appraisals_dataset.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Files - Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import config\n",
    "import utils\n",
    "\n",
    "\n",
    "def load_appraisals_data(file_name=config.RAW_DATA_FILE):\n",
    "    \"\"\"Loads the appraisals dataset from a JSON file located within src/.\n",
    "\n",
    "    Args:\n",
    "        file_name (str, optional): The base name of the data file.\n",
    "                                     Defaults to config.RAW_DATA_FILE.\n",
    "                                     The path is constructed relative to config.py's location.\n",
    "    \"\"\"\n",
    "    # Construct path relative to the directory of config.py (i.e., src/)\n",
    "    # This ensures it works correctly whether called from src/main.py or a script in root.\n",
    "    base_src_dir = os.path.dirname(config.__file__)\n",
    "    full_file_path = os.path.join(base_src_dir, file_name)\n",
    "\n",
    "    print(f\"Loading {full_file_path}...\")\n",
    "    try:\n",
    "        with open(full_file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # If the loaded data is a dictionary and has an 'appraisals' key,\n",
    "        # assume the actual list of appraisals is nested there.\n",
    "        if isinstance(data, dict) and 'appraisals' in data:\n",
    "            data = data['appraisals']\n",
    "            print(\n",
    "                \"Note: Data was unwrapped from an 'appraisals' key in the JSON structure.\")\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            print(f\"Successfully loaded {len(data)} appraisals.\")\n",
    "            return data\n",
    "        else:\n",
    "            # This case should be rare if the primary structure is a list or dict with 'appraisals'\n",
    "            error_msg = f\"Error: Loaded data from {full_file_path} is not a list of appraisals as expected. Type found: {type(data)}.\"\n",
    "            if hasattr(data, '__len__'):\n",
    "                error_msg += f\" Number of top-level elements: {len(data)}.\"\n",
    "            else:\n",
    "                error_msg += \" Data does not have a defined length.\"\n",
    "            print(error_msg)\n",
    "            # Decide on behavior: return None, raise error, or return data with warning\n",
    "            # For now, returning None as the original error paths did.\n",
    "            return None\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {full_file_path} was not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: The file {full_file_path} is not a valid JSON file.\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential errors during file/JSON processing\n",
    "        print(\n",
    "            f\"An unexpected error occurred while loading {full_file_path}: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_initial_eda(appraisals_data):\n",
    "    \"\"\"Prints initial exploratory data analysis insights.\"\"\"\n",
    "    if not appraisals_data:\n",
    "        print(\"No data to perform EDA on.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Exploratory Data Analysis ---\")\n",
    "    print(f\"Total number of appraisals: {len(appraisals_data)}\")\n",
    "\n",
    "    if appraisals_data:\n",
    "        first_appraisal = appraisals_data[0]\n",
    "        print(\"\\nStructure of the first appraisal (keys):\")\n",
    "        print(list(first_appraisal.keys()))\n",
    "\n",
    "        subject_prop = first_appraisal.get('subject', {})\n",
    "        print(\"\\nKeys in the 'subject' property of the first appraisal:\")\n",
    "        print(list(subject_prop.keys()))\n",
    "        print(f\"Subject property address: {subject_prop.get('address')}\")\n",
    "        print(f\"Subject GLA: {subject_prop.get('gla')}, \"\n",
    "              f\"Beds: {subject_prop.get('num_beds')}, \"\n",
    "              f\"Baths: {subject_prop.get('num_baths')}\")\n",
    "\n",
    "        comps = first_appraisal.get('comps', [])\n",
    "        print(\n",
    "            f\"\\nNumber of chosen comparables (comps) in the first appraisal: {len(comps)}\")\n",
    "        if comps:\n",
    "            print(\"Keys in the first chosen 'comp':\")\n",
    "            print(list(comps[0].keys()))\n",
    "            print(f\"First chosen comp address: {comps[0].get('address')}\")\n",
    "\n",
    "        properties = first_appraisal.get('properties', [])\n",
    "        print(\n",
    "            f\"\\nNumber of potential comparables ('properties') in the first appraisal: {len(properties)}\")\n",
    "        if properties:\n",
    "            print(\"Keys in the first 'property' from the potential list:\")\n",
    "            print(list(properties[0].keys()))\n",
    "\n",
    "    # Statistics for 'properties' list per appraisal\n",
    "    num_properties_list = [len(appraisal.get('properties', []))\n",
    "                           for appraisal in appraisals_data]\n",
    "    if num_properties_list:\n",
    "        print(\"\\nStatistics for 'properties' list per appraisal:\")\n",
    "        s_num_properties = pd.Series(num_properties_list)\n",
    "        print(f\"  Min: {s_num_properties.min()}\")\n",
    "        print(f\"  Max: {s_num_properties.max()}\")\n",
    "        print(f\"  Avg: {s_num_properties.mean():.2f}\")\n",
    "        print(f\"  Median: {s_num_properties.median()}\")\n",
    "\n",
    "    # Distribution of chosen 'comps' (should always be 3 based on prior analysis)\n",
    "    num_comps_list = [len(appraisal.get('comps', []))\n",
    "                      for appraisal in appraisals_data]\n",
    "    if num_comps_list:\n",
    "        print(\"\\nDistribution of number of chosen 'comps' per appraisal:\")\n",
    "        print(pd.Series(num_comps_list).value_counts())\n",
    "\n",
    "    # Show subject GLA missingness\n",
    "    subject_gla_missing_count = 0\n",
    "    for appraisal in appraisals_data:\n",
    "        if pd.isna(utils.safe_float(appraisal.get('subject', {}).get('gla'))):\n",
    "            subject_gla_missing_count += 1\n",
    "    print(\n",
    "        f\"\\nNumber of appraisals with missing subject_gla: {subject_gla_missing_count} out of {len(appraisals_data)}\")\n",
    "\n",
    "    # Show subject lot_size_sf missingness\n",
    "    subject_lot_sf_missing_count = 0\n",
    "    for appraisal in appraisals_data:\n",
    "        if pd.isna(utils.safe_float(appraisal.get('subject', {}).get('lot_size_sf'))):\n",
    "            subject_lot_sf_missing_count += 1\n",
    "    print(\n",
    "        f\"Number of appraisals with missing subject_lot_sf: {subject_lot_sf_missing_count} out of {len(appraisals_data)}\")\n",
    "\n",
    "    # Show subject lat/lon missingness\n",
    "    subject_lat_lon_missing = 0\n",
    "    for appraisal in appraisals_data:\n",
    "        subj = appraisal.get('subject', {})\n",
    "        if pd.isna(utils.safe_float(subj.get('latitude'))) or pd.isna(utils.safe_float(subj.get('longitude'))):\n",
    "            subject_lat_lon_missing += 1\n",
    "    print(\n",
    "        f\"Number of subjects missing direct lat/lon: {subject_lat_lon_missing} out of {len(appraisals_data)}\")\n",
    "\n",
    "    # Matches the original script's output separator\n",
    "    print(\"\\n--- End of Initial EDA --- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/haroon/Projects /Headstarter/CompRecommendation/src/appraisals_dataset.json...\n",
      "Note: Data was unwrapped from an 'appraisals' key in the JSON structure.\n",
      "Successfully loaded 88 appraisals.\n"
     ]
    }
   ],
   "source": [
    "appraisals_data = load_appraisals_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded geocoding cache from /Users/haroon/Projects /Headstarter/CompRecommendation/src/geocoding_cache.json with 88 entries.\n"
     ]
    }
   ],
   "source": [
    "import geocoding_utils\n",
    "import feature_engineering\n",
    "# 3. Load Geocoding Cache\n",
    "    # Now uses default file_name from config, path constructed within the function\n",
    "geocoding_cache = geocoding_utils.load_geocoding_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing appraisals to create features...\n",
      "Value counts for is_chosen_comp directly after DataFrame creation:\n",
      "is_chosen_comp\n",
      "0    9717\n",
      "1     103\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Created DataFrame with 9820 rows and 38 columns.\n",
      "\n",
      "Imputing medians for specific diff features and creating missingness indicators...\n",
      "  Processed column: bed_diff. Missing: 386, Median used: 1.0\n",
      "  Processed column: room_diff. Missing: 169, Median used: 2.0\n",
      "Processing column: distance_to_subject\n",
      "  Created 'distance_to_subject_missing' indicator. Median used: 1.381468883835596\n",
      "Processing column: bath_diff\n",
      "  Created 'bath_diff_missing' indicator column. Median used: 0.5\n",
      "Processing column: age_diff\n",
      "  Created 'age_diff_missing' indicator column. Median used: 13.0\n",
      "\n",
      "Checking and imputing interaction/polynomial features...\n",
      "  Imputed 2937 NaNs in 'distance_squared' with 0.0.\n",
      "  Imputed 4150 NaNs in 'age_diff_squared' with 0.0.\n",
      "  Imputed 5467 NaNs in 'dist_X_age_diff' with 0.0.\n",
      "\n",
      "--- End of Feature Engineering ---\n"
     ]
    }
   ],
   "source": [
    "# 4. Feature Engineering\n",
    "    # Pass the cache, and get the (potentially updated) cache back\n",
    "df_features, geocoding_cache = feature_engineering.create_feature_dataframe(\n",
    "    appraisals_data, geocoding_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved geocoding cache to /Users/haroon/Projects /Headstarter/CompRecommendation/src/geocoding_cache.json with 88 entries.\n"
     ]
    }
   ],
   "source": [
    "# 5. Save updated geocoding cache\n",
    "    # Now uses default file_name from config, path constructed within the function\n",
    "geocoding_utils.save_geocoding_cache(geocoding_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Engineered Features Description (Post-Imputation) ---\n",
      "                              count        mean          std        min        25%         50%         75%           max\n",
      "days_since_sale              9820.0   41.321181    32.467724   0.000000   15.00000   33.000000   64.000000    180.000000\n",
      "bed_diff                     9820.0    1.141039     1.293070   0.000000    0.00000    1.000000    1.000000      7.000000\n",
      "bath_diff                    9820.0    0.593941     0.405004   0.000000    0.50000    0.500000    0.500000      8.500000\n",
      "age_diff                     9820.0   18.742770    19.394029   0.000000   10.00000   13.000000   17.000000    120.000000\n",
      "room_diff                    9820.0    3.157637     2.752957   0.000000    1.00000    2.000000    4.000000     20.000000\n",
      "distance_to_subject          9820.0    1.506970     1.074992   0.003466    1.10727    1.381469    1.615361     18.350098\n",
      "gla_diff                      139.0  508.410072   420.287735  56.000000  256.00000  256.000000  706.000000   1770.000000\n",
      "lot_sf_diff                     0.0         NaN          NaN        NaN        NaN         NaN         NaN           NaN\n",
      "bath_diff_missing            9820.0    0.714664     0.451597   0.000000    0.00000    1.000000    1.000000      1.000000\n",
      "age_diff_missing             9820.0    0.422607     0.493999   0.000000    0.00000    0.000000    1.000000      1.000000\n",
      "distance_to_subject_missing  9820.0    0.299084     0.457880   0.000000    0.00000    0.000000    1.000000      1.000000\n",
      "distance_squared             9820.0    2.855661    10.303421   0.000000    0.00000    0.948824    2.609392    336.726099\n",
      "age_diff_squared             9820.0  655.960896  1832.261059   0.000000    0.00000    4.000000  289.000000  14400.000000\n",
      "dist_X_age_diff              9820.0   16.446043    47.892344   0.000000    0.00000    0.000000   11.699091   1271.369991\n",
      "fsa_X_days_since_sale        9820.0   23.384216    31.423299   0.000000    0.00000    7.000000   41.000000    180.000000\n",
      "\n",
      "Missing values summary after all feature engineering and imputation:\n",
      "lot_sf_diff       9820\n",
      "subject_lot_sf    9820\n",
      "gla_diff          9681\n",
      "gla_ratio         9681\n",
      "subject_gla       9674\n",
      "prop_lot_sf       4876\n",
      "prop_age          4026\n",
      "subject_baths     3761\n",
      "prop_baths        3424\n",
      "subject_lat       2937\n",
      "subject_lon       2937\n",
      "subject_age        761\n",
      "subject_beds       213\n",
      "prop_gla           176\n",
      "prop_beds          173\n",
      "\n",
      "Distribution of target 'is_chosen_comp':\n",
      "is_chosen_comp\n",
      "0    98.95112\n",
      "1     1.04888\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Value counts of target 'is_chosen_comp':\n",
      "is_chosen_comp\n",
      "0    9717\n",
      "1     103\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- End of Phase 2 Data Preprocessing --- \n"
     ]
    }
   ],
   "source": [
    "# 6. Describe Engineered Features (Optional)\n",
    "feature_engineering.describe_engineered_features(df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Starting XGBoost Model Training and Evaluation ---\n",
      "\n",
      "Shape of X (features): (9820, 12)\n",
      "Shape of y (target): (9820,)\n",
      "\n",
      "Performing stratified train/test split...\n",
      "Shape of X_train: (7856, 12), y_train: (7856,)\n",
      "Shape of X_test: (1964, 12), y_test: (1964,)\n",
      "Distribution of target in y_train:\n",
      "is_chosen_comp\n",
      "0    98.956212\n",
      "1     1.043788\n",
      "Name: proportion, dtype: float64\n",
      "Distribution of target in y_test:\n",
      "is_chosen_comp\n",
      "0    98.930754\n",
      "1     1.069246\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Scaling features using StandardScaler...\n",
      "Calculated scale_pos_weight: 94.80\n",
      "\n",
      "Training XGBoost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haroon/Projects /Headstarter/CompRecommendation/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [16:48:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost model training complete.\n",
      "\n",
      "Evaluating XGBoost model on the test set (default 0.5 threshold)...\n",
      "\n",
      "Classification Report (0.5 threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1943\n",
      "           1       0.44      0.33      0.38        21\n",
      "\n",
      "    accuracy                           0.99      1964\n",
      "   macro avg       0.72      0.66      0.69      1964\n",
      "weighted avg       0.99      0.99      0.99      1964\n",
      "\n",
      "\n",
      "Average Precision Score (AUPRC) for XGBoost: 0.2987\n",
      "\n",
      "Feature Importances (XGBoost - Gain/Weight):\n",
      "prop_sold_after_eff            0.480706\n",
      "fsa_match                      0.175935\n",
      "distance_to_subject            0.054616\n",
      "bath_diff                      0.050126\n",
      "age_diff                       0.049035\n",
      "bed_diff_missing               0.032142\n",
      "distance_to_subject_missing    0.030815\n",
      "days_since_sale                0.028803\n",
      "age_diff_missing               0.028596\n",
      "bed_diff                       0.027028\n",
      "struct_type_match              0.022208\n",
      "bath_diff_missing              0.019989\n",
      "dtype: float32\n",
      "\n",
      "--- Starting Threshold Tuning for Positive Class (1) - XGBoost ---\n",
      "Threshold | Precision (1) | Recall (1) | F1-score (1)\n",
      "-----------------------------------------------------\n",
      "     0.01 |        0.0433 |     0.4286 |       0.0786\n",
      "     0.02 |        0.0541 |     0.3810 |       0.0947\n",
      "     0.03 |        0.0661 |     0.3810 |       0.1127\n",
      "     0.04 |        0.0762 |     0.3810 |       0.1270\n",
      "     0.05 |        0.0808 |     0.3810 |       0.1333\n",
      "     0.06 |        0.0909 |     0.3810 |       0.1468\n",
      "     0.07 |        0.0964 |     0.3810 |       0.1538\n",
      "     0.08 |        0.1053 |     0.3810 |       0.1649\n",
      "     0.09 |        0.1159 |     0.3810 |       0.1778\n",
      "     0.10 |        0.1290 |     0.3810 |       0.1928\n",
      "     0.11 |        0.1207 |     0.3333 |       0.1772\n",
      "     0.12 |        0.1296 |     0.3333 |       0.1867\n",
      "     0.13 |        0.1321 |     0.3333 |       0.1892\n",
      "     0.14 |        0.1429 |     0.3333 |       0.2000\n",
      "     0.15 |        0.1429 |     0.3333 |       0.2000\n",
      "     0.16 |        0.1522 |     0.3333 |       0.2090\n",
      "     0.17 |        0.1591 |     0.3333 |       0.2154\n",
      "     0.18 |        0.1842 |     0.3333 |       0.2373\n",
      "     0.19 |        0.1892 |     0.3333 |       0.2414\n",
      "     0.20 |        0.2059 |     0.3333 |       0.2545\n",
      "     0.21 |        0.2188 |     0.3333 |       0.2642\n",
      "     0.22 |        0.2258 |     0.3333 |       0.2692\n",
      "     0.23 |        0.2333 |     0.3333 |       0.2745\n",
      "     0.24 |        0.2333 |     0.3333 |       0.2745\n",
      "     0.25 |        0.2500 |     0.3333 |       0.2857\n",
      "     0.26 |        0.2593 |     0.3333 |       0.2917\n",
      "     0.27 |        0.2692 |     0.3333 |       0.2979\n",
      "     0.28 |        0.2692 |     0.3333 |       0.2979\n",
      "     0.29 |        0.2800 |     0.3333 |       0.3043\n",
      "     0.30 |        0.2800 |     0.3333 |       0.3043\n",
      "     0.31 |        0.2917 |     0.3333 |       0.3111\n",
      "     0.32 |        0.3043 |     0.3333 |       0.3182\n",
      "     0.33 |        0.3333 |     0.3333 |       0.3333\n",
      "     0.34 |        0.3500 |     0.3333 |       0.3415\n",
      "     0.35 |        0.3500 |     0.3333 |       0.3415\n",
      "     0.36 |        0.3500 |     0.3333 |       0.3415\n",
      "     0.37 |        0.3684 |     0.3333 |       0.3500\n",
      "     0.38 |        0.3684 |     0.3333 |       0.3500\n",
      "     0.39 |        0.3889 |     0.3333 |       0.3590\n",
      "     0.40 |        0.3889 |     0.3333 |       0.3590\n",
      "     0.41 |        0.3889 |     0.3333 |       0.3590\n",
      "     0.42 |        0.3889 |     0.3333 |       0.3590\n",
      "     0.43 |        0.4118 |     0.3333 |       0.3684\n",
      "     0.44 |        0.4118 |     0.3333 |       0.3684\n",
      "     0.45 |        0.4118 |     0.3333 |       0.3684\n",
      "     0.46 |        0.4118 |     0.3333 |       0.3684\n",
      "     0.47 |        0.4375 |     0.3333 |       0.3784\n",
      "     0.48 |        0.4375 |     0.3333 |       0.3784\n",
      "     0.49 |        0.4375 |     0.3333 |       0.3784\n",
      "     0.50 |        0.4375 |     0.3333 |       0.3784\n",
      "     0.51 |        0.4375 |     0.3333 |       0.3784\n",
      "     0.52 |        0.4286 |     0.2857 |       0.3429\n",
      "     0.53 |        0.4615 |     0.2857 |       0.3529\n",
      "     0.54 |        0.4615 |     0.2857 |       0.3529\n",
      "     0.55 |        0.5000 |     0.2857 |       0.3636\n",
      "     0.56 |        0.5000 |     0.2857 |       0.3636\n",
      "     0.57 |        0.5000 |     0.2857 |       0.3636\n",
      "     0.58 |        0.5455 |     0.2857 |       0.3750\n",
      "     0.59 |        0.5455 |     0.2857 |       0.3750\n",
      "-----------------------------------------------------\n",
      "Best F1-score (Positive Class) for XGBoost: 0.3784 at threshold 0.47\n",
      "  Precision (Positive Class) at best F1 threshold: 0.4375\n",
      "  Recall (Positive Class) at best F1 threshold: 0.3333\n",
      "\n",
      "Classification Report for XGBoost with Best F1 Threshold (0.47):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1943\n",
      "           1       0.44      0.33      0.38        21\n",
      "\n",
      "    accuracy                           0.99      1964\n",
      "   macro avg       0.72      0.66      0.69      1964\n",
      "weighted avg       0.99      0.99      0.99      1964\n",
      "\n",
      "--- End of XGBoost Model Training and Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "model_pipeline.train_evaluate_model(df_features, model_name='XGBoost')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Starting LightGBM Model Training and Evaluation ---\n",
      "\n",
      "Shape of X (features): (9820, 12)\n",
      "Shape of y (target): (9820,)\n",
      "\n",
      "Performing stratified train/test split...\n",
      "Shape of X_train: (7856, 12), y_train: (7856,)\n",
      "Shape of X_test: (1964, 12), y_test: (1964,)\n",
      "Distribution of target in y_train:\n",
      "is_chosen_comp\n",
      "0    98.956212\n",
      "1     1.043788\n",
      "Name: proportion, dtype: float64\n",
      "Distribution of target in y_test:\n",
      "is_chosen_comp\n",
      "0    98.930754\n",
      "1     1.069246\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Scaling features using StandardScaler...\n",
      "Calculated scale_pos_weight: 94.80\n",
      "\n",
      "Training LightGBM model...\n",
      "[LightGBM] [Info] Number of positive: 82, number of negative: 7774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 558\n",
      "[LightGBM] [Info] Number of data points in the train set: 7856, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010438 -> initscore=-4.551821\n",
      "[LightGBM] [Info] Start training from score -4.551821\n",
      "LightGBM model training complete.\n",
      "\n",
      "Evaluating LightGBM model on the test set (default 0.5 threshold)...\n",
      "\n",
      "Classification Report (0.5 threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1943\n",
      "           1       0.33      0.29      0.31        21\n",
      "\n",
      "    accuracy                           0.99      1964\n",
      "   macro avg       0.66      0.64      0.65      1964\n",
      "weighted avg       0.99      0.99      0.99      1964\n",
      "\n",
      "\n",
      "Average Precision Score (AUPRC) for LightGBM: 0.2825\n",
      "\n",
      "Feature Importances (LightGBM - Gain/Weight):\n",
      "distance_to_subject            979\n",
      "days_since_sale                959\n",
      "age_diff                       383\n",
      "bed_diff                       184\n",
      "bath_diff                      138\n",
      "age_diff_missing                66\n",
      "struct_type_match               63\n",
      "bath_diff_missing               61\n",
      "fsa_match                       61\n",
      "bed_diff_missing                50\n",
      "prop_sold_after_eff             36\n",
      "distance_to_subject_missing     20\n",
      "dtype: int32\n",
      "\n",
      "--- Starting Threshold Tuning for Positive Class (1) - LightGBM ---\n",
      "Threshold | Precision (1) | Recall (1) | F1-score (1)\n",
      "-----------------------------------------------------\n",
      "     0.01 |        0.0545 |     0.4286 |       0.0968\n",
      "     0.02 |        0.0661 |     0.3810 |       0.1127\n",
      "     0.03 |        0.0784 |     0.3810 |       0.1301\n",
      "     0.04 |        0.0851 |     0.3810 |       0.1391\n",
      "     0.05 |        0.0909 |     0.3810 |       0.1468\n",
      "     0.06 |        0.0952 |     0.3810 |       0.1524\n",
      "     0.07 |        0.1026 |     0.3810 |       0.1616\n",
      "     0.08 |        0.1111 |     0.3810 |       0.1720\n",
      "     0.09 |        0.1159 |     0.3810 |       0.1778\n",
      "     0.10 |        0.1231 |     0.3810 |       0.1860\n",
      "     0.11 |        0.1250 |     0.3810 |       0.1882\n",
      "     0.12 |        0.1311 |     0.3810 |       0.1951\n",
      "     0.13 |        0.1379 |     0.3810 |       0.2025\n",
      "     0.14 |        0.1538 |     0.3810 |       0.2192\n",
      "     0.15 |        0.1633 |     0.3810 |       0.2286\n",
      "     0.16 |        0.1739 |     0.3810 |       0.2388\n",
      "     0.17 |        0.1818 |     0.3810 |       0.2462\n",
      "     0.18 |        0.1707 |     0.3333 |       0.2258\n",
      "     0.19 |        0.1795 |     0.3333 |       0.2333\n",
      "     0.20 |        0.1795 |     0.3333 |       0.2333\n",
      "     0.21 |        0.1842 |     0.3333 |       0.2373\n",
      "     0.22 |        0.1842 |     0.3333 |       0.2373\n",
      "     0.23 |        0.1944 |     0.3333 |       0.2456\n",
      "     0.24 |        0.2121 |     0.3333 |       0.2593\n",
      "     0.25 |        0.2414 |     0.3333 |       0.2800\n",
      "     0.26 |        0.2414 |     0.3333 |       0.2800\n",
      "     0.27 |        0.2500 |     0.3333 |       0.2857\n",
      "     0.28 |        0.2500 |     0.3333 |       0.2857\n",
      "     0.29 |        0.2692 |     0.3333 |       0.2979\n",
      "     0.30 |        0.2800 |     0.3333 |       0.3043\n",
      "     0.31 |        0.2800 |     0.3333 |       0.3043\n",
      "     0.32 |        0.2917 |     0.3333 |       0.3111\n",
      "     0.33 |        0.2917 |     0.3333 |       0.3111\n",
      "     0.34 |        0.2917 |     0.3333 |       0.3111\n",
      "     0.35 |        0.2917 |     0.3333 |       0.3111\n",
      "     0.36 |        0.3043 |     0.3333 |       0.3182\n",
      "     0.37 |        0.2727 |     0.2857 |       0.2791\n",
      "     0.38 |        0.2857 |     0.2857 |       0.2857\n",
      "     0.39 |        0.2857 |     0.2857 |       0.2857\n",
      "     0.40 |        0.2857 |     0.2857 |       0.2857\n",
      "     0.41 |        0.2857 |     0.2857 |       0.2857\n",
      "     0.42 |        0.2857 |     0.2857 |       0.2857\n",
      "     0.43 |        0.3000 |     0.2857 |       0.2927\n",
      "     0.44 |        0.3000 |     0.2857 |       0.2927\n",
      "     0.45 |        0.3000 |     0.2857 |       0.2927\n",
      "     0.46 |        0.3000 |     0.2857 |       0.2927\n",
      "     0.47 |        0.3000 |     0.2857 |       0.2927\n",
      "     0.48 |        0.3000 |     0.2857 |       0.2927\n",
      "     0.49 |        0.3158 |     0.2857 |       0.3000\n",
      "     0.50 |        0.3333 |     0.2857 |       0.3077\n",
      "     0.51 |        0.3333 |     0.2857 |       0.3077\n",
      "     0.52 |        0.3333 |     0.2857 |       0.3077\n",
      "     0.53 |        0.3333 |     0.2857 |       0.3077\n",
      "     0.54 |        0.3529 |     0.2857 |       0.3158\n",
      "     0.55 |        0.3529 |     0.2857 |       0.3158\n",
      "     0.56 |        0.3750 |     0.2857 |       0.3243\n",
      "     0.57 |        0.3750 |     0.2857 |       0.3243\n",
      "     0.58 |        0.3750 |     0.2857 |       0.3243\n",
      "     0.59 |        0.3750 |     0.2857 |       0.3243\n",
      "-----------------------------------------------------\n",
      "Best F1-score (Positive Class) for LightGBM: 0.3243 at threshold 0.56\n",
      "  Precision (Positive Class) at best F1 threshold: 0.3750\n",
      "  Recall (Positive Class) at best F1 threshold: 0.2857\n",
      "\n",
      "Classification Report for LightGBM with Best F1 Threshold (0.56):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1943\n",
      "           1       0.38      0.29      0.32        21\n",
      "\n",
      "    accuracy                           0.99      1964\n",
      "   macro avg       0.68      0.64      0.66      1964\n",
      "weighted avg       0.99      0.99      0.99      1964\n",
      "\n",
      "--- End of LightGBM Model Training and Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "model_pipeline.train_evaluate_model(df_features, model_name='LightGBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Starting LogisticRegression Model Training and Evaluation ---\n",
      "\n",
      "Shape of X (features): (9820, 12)\n",
      "Shape of y (target): (9820,)\n",
      "\n",
      "Performing stratified train/test split...\n",
      "Shape of X_train: (7856, 12), y_train: (7856,)\n",
      "Shape of X_test: (1964, 12), y_test: (1964,)\n",
      "Distribution of target in y_train:\n",
      "is_chosen_comp\n",
      "0    98.956212\n",
      "1     1.043788\n",
      "Name: proportion, dtype: float64\n",
      "Distribution of target in y_test:\n",
      "is_chosen_comp\n",
      "0    98.930754\n",
      "1     1.069246\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Scaling features using StandardScaler...\n",
      "Calculated scale_pos_weight: 94.80\n",
      "\n",
      "Training Logistic Regression model...\n",
      "LogisticRegression model training complete.\n",
      "\n",
      "Evaluating LogisticRegression model on the test set (default 0.5 threshold)...\n",
      "\n",
      "Classification Report (0.5 threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80      1943\n",
      "           1       0.03      0.90      0.06        21\n",
      "\n",
      "    accuracy                           0.68      1964\n",
      "   macro avg       0.51      0.79      0.43      1964\n",
      "weighted avg       0.99      0.68      0.80      1964\n",
      "\n",
      "\n",
      "Average Precision Score (AUPRC) for LogisticRegression: 0.0620\n",
      "\n",
      "Feature Importances (LogisticRegression - Gain/Weight):\n",
      "prop_sold_after_eff            2.669145\n",
      "fsa_match                      0.655006\n",
      "distance_to_subject            0.616911\n",
      "bath_diff_missing              0.388998\n",
      "bath_diff                      0.292571\n",
      "distance_to_subject_missing    0.277156\n",
      "bed_diff                       0.275835\n",
      "age_diff_missing               0.268509\n",
      "age_diff                       0.231726\n",
      "struct_type_match              0.229615\n",
      "days_since_sale                0.228826\n",
      "bed_diff_missing               0.042776\n",
      "dtype: float64\n",
      "\n",
      "(Note: For Logistic Regression, these are coefficient magnitudes)\n",
      "\n",
      "--- Starting Threshold Tuning for Positive Class (1) - LogisticRegression ---\n",
      "Threshold | Precision (1) | Recall (1) | F1-score (1)\n",
      "-----------------------------------------------------\n",
      "     0.01 |        0.0142 |     1.0000 |       0.0280\n",
      "     0.02 |        0.0142 |     1.0000 |       0.0281\n",
      "     0.03 |        0.0143 |     1.0000 |       0.0282\n",
      "     0.04 |        0.0144 |     1.0000 |       0.0283\n",
      "     0.05 |        0.0145 |     1.0000 |       0.0285\n",
      "     0.06 |        0.0147 |     1.0000 |       0.0289\n",
      "     0.07 |        0.0148 |     1.0000 |       0.0292\n",
      "     0.08 |        0.0150 |     1.0000 |       0.0296\n",
      "     0.09 |        0.0152 |     1.0000 |       0.0300\n",
      "     0.10 |        0.0154 |     1.0000 |       0.0303\n",
      "     0.11 |        0.0157 |     1.0000 |       0.0309\n",
      "     0.12 |        0.0159 |     1.0000 |       0.0313\n",
      "     0.13 |        0.0162 |     1.0000 |       0.0319\n",
      "     0.14 |        0.0165 |     1.0000 |       0.0325\n",
      "     0.15 |        0.0167 |     1.0000 |       0.0329\n",
      "     0.16 |        0.0170 |     1.0000 |       0.0335\n",
      "     0.17 |        0.0173 |     1.0000 |       0.0340\n",
      "     0.18 |        0.0177 |     1.0000 |       0.0347\n",
      "     0.19 |        0.0180 |     1.0000 |       0.0353\n",
      "     0.20 |        0.0183 |     1.0000 |       0.0360\n",
      "     0.21 |        0.0188 |     1.0000 |       0.0369\n",
      "     0.22 |        0.0192 |     1.0000 |       0.0378\n",
      "     0.23 |        0.0195 |     1.0000 |       0.0383\n",
      "     0.24 |        0.0200 |     1.0000 |       0.0392\n",
      "     0.25 |        0.0204 |     1.0000 |       0.0401\n",
      "     0.26 |        0.0209 |     1.0000 |       0.0410\n",
      "     0.27 |        0.0214 |     1.0000 |       0.0418\n",
      "     0.28 |        0.0218 |     1.0000 |       0.0427\n",
      "     0.29 |        0.0222 |     1.0000 |       0.0434\n",
      "     0.30 |        0.0224 |     1.0000 |       0.0438\n",
      "     0.31 |        0.0227 |     1.0000 |       0.0443\n",
      "     0.32 |        0.0220 |     0.9524 |       0.0431\n",
      "     0.33 |        0.0223 |     0.9524 |       0.0437\n",
      "     0.34 |        0.0228 |     0.9524 |       0.0444\n",
      "     0.35 |        0.0231 |     0.9524 |       0.0451\n",
      "     0.36 |        0.0234 |     0.9524 |       0.0456\n",
      "     0.37 |        0.0238 |     0.9524 |       0.0465\n",
      "     0.38 |        0.0242 |     0.9524 |       0.0472\n",
      "     0.39 |        0.0247 |     0.9524 |       0.0481\n",
      "     0.40 |        0.0251 |     0.9524 |       0.0488\n",
      "     0.41 |        0.0254 |     0.9524 |       0.0495\n",
      "     0.42 |        0.0245 |     0.9048 |       0.0477\n",
      "     0.43 |        0.0249 |     0.9048 |       0.0484\n",
      "     0.44 |        0.0253 |     0.9048 |       0.0492\n",
      "     0.45 |        0.0259 |     0.9048 |       0.0503\n",
      "     0.46 |        0.0265 |     0.9048 |       0.0514\n",
      "     0.47 |        0.0271 |     0.9048 |       0.0526\n",
      "     0.48 |        0.0277 |     0.9048 |       0.0537\n",
      "     0.49 |        0.0281 |     0.9048 |       0.0544\n",
      "     0.50 |        0.0290 |     0.9048 |       0.0562\n",
      "     0.51 |        0.0280 |     0.8571 |       0.0543\n",
      "     0.52 |        0.0290 |     0.8571 |       0.0561\n",
      "     0.53 |        0.0297 |     0.8571 |       0.0574\n",
      "     0.54 |        0.0305 |     0.8571 |       0.0588\n",
      "     0.55 |        0.0296 |     0.8095 |       0.0570\n",
      "     0.56 |        0.0305 |     0.8095 |       0.0587\n",
      "     0.57 |        0.0317 |     0.8095 |       0.0609\n",
      "     0.58 |        0.0309 |     0.7619 |       0.0594\n",
      "     0.59 |        0.0317 |     0.7619 |       0.0610\n",
      "-----------------------------------------------------\n",
      "Best F1-score (Positive Class) for LogisticRegression: 0.0610 at threshold 0.59\n",
      "  Precision (Positive Class) at best F1 threshold: 0.0317\n",
      "  Recall (Positive Class) at best F1 threshold: 0.7619\n",
      "\n",
      "Classification Report for LogisticRegression with Best F1 Threshold (0.59):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86      1943\n",
      "           1       0.03      0.76      0.06        21\n",
      "\n",
      "    accuracy                           0.75      1964\n",
      "   macro avg       0.51      0.76      0.46      1964\n",
      "weighted avg       0.99      0.75      0.85      1964\n",
      "\n",
      "--- End of LogisticRegression Model Training and Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Logistic Regression\n",
    "model_pipeline.train_evaluate_model(\n",
    "df_features, model_name='LogisticRegression')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Starting KNN Model Training and Evaluation ---\n",
      "\n",
      "Shape of X (features): (9820, 12)\n",
      "Shape of y (target): (9820,)\n",
      "\n",
      "Performing stratified train/test split...\n",
      "Shape of X_train: (7856, 12), y_train: (7856,)\n",
      "Shape of X_test: (1964, 12), y_test: (1964,)\n",
      "Distribution of target in y_train:\n",
      "is_chosen_comp\n",
      "0    98.956212\n",
      "1     1.043788\n",
      "Name: proportion, dtype: float64\n",
      "Distribution of target in y_test:\n",
      "is_chosen_comp\n",
      "0    98.930754\n",
      "1     1.069246\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Attempting SMOTE for KNN training data...\n",
      "Applying SMOTE with k_neighbors=5 for KNN.\n",
      "Shape of X_train after SMOTE: (15548, 12)\n",
      "Distribution of target in y_train after SMOTE:\n",
      "is_chosen_comp\n",
      "0    50.0\n",
      "1    50.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Scaling features using StandardScaler...\n",
      "Calculated scale_pos_weight: 94.80\n",
      "\n",
      "Training KNN model...\n",
      "KNN model training complete.\n",
      "\n",
      "Evaluating KNN model on the test set (default 0.5 threshold)...\n",
      "\n",
      "Classification Report (0.5 threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.96      1943\n",
      "           1       0.04      0.29      0.07        21\n",
      "\n",
      "    accuracy                           0.91      1964\n",
      "   macro avg       0.51      0.60      0.51      1964\n",
      "weighted avg       0.98      0.91      0.95      1964\n",
      "\n",
      "\n",
      "Average Precision Score (AUPRC) for KNN: 0.0218\n",
      "\n",
      "Feature Importances (KNN - Gain/Weight):\n",
      "KNN does not provide direct feature importances in the same way as other models.\n",
      "\n",
      "--- Starting Threshold Tuning for Positive Class (1) - KNN ---\n",
      "Threshold | Precision (1) | Recall (1) | F1-score (1)\n",
      "-----------------------------------------------------\n",
      "     0.01 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.02 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.03 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.04 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.05 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.06 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.07 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.08 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.09 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.10 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.11 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.12 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.13 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.14 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.15 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.16 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.17 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.18 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.19 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.20 |        0.0254 |     0.3333 |       0.0471\n",
      "     0.21 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.22 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.23 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.24 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.25 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.26 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.27 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.28 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.29 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.30 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.31 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.32 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.33 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.34 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.35 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.36 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.37 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.38 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.39 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.40 |        0.0312 |     0.3333 |       0.0571\n",
      "     0.41 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.42 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.43 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.44 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.45 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.46 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.47 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.48 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.49 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.50 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.51 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.52 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.53 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.54 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.55 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.56 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.57 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.58 |        0.0380 |     0.2857 |       0.0670\n",
      "     0.59 |        0.0380 |     0.2857 |       0.0670\n",
      "-----------------------------------------------------\n",
      "Best F1-score (Positive Class) for KNN: 0.0670 at threshold 0.41\n",
      "  Precision (Positive Class) at best F1 threshold: 0.0380\n",
      "  Recall (Positive Class) at best F1 threshold: 0.2857\n",
      "\n",
      "Classification Report for KNN with Best F1 Threshold (0.41):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.96      1943\n",
      "           1       0.04      0.29      0.07        21\n",
      "\n",
      "    accuracy                           0.91      1964\n",
      "   macro avg       0.51      0.60      0.51      1964\n",
      "weighted avg       0.98      0.91      0.95      1964\n",
      "\n",
      "--- End of KNN Model Training and Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "model_pipeline.train_evaluate_model(df_features, model_name='KNN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
