{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ ML Training Pipeline - Learning from Real Appraisers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Load Data and Understand Structure\n",
    "Load both the real appraiser selections (ground truth) and our engineered features dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading datasets...\n",
      "Real appraiser selections: 264 comparables\n",
      "All property-subject pairs: 7,246 pairs\n",
      "Subjects in comps: 88\n",
      "Subjects in properties: 88\n",
      "\n",
      "Comps columns: ['subject_id', 'comp_id', 'comp_index', 'condition', 'age_years']...\n",
      "Properties columns: ['property_id', 'subject_id', 'orderID', 'structure_type', 'property_sub_type']...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the datasets\n",
    "print(\"üìä Loading datasets...\")\n",
    "comps_df = pd.read_csv('data/processed/comps_cleaned_with_subjects.csv')\n",
    "properties_df = pd.read_csv('data/processed/properties_comparison_engineered.csv')\n",
    "subject_df = pd.read_csv('data/processed/subjects_cleaned.csv')\n",
    "\n",
    "print(f\"Real appraiser selections: {len(comps_df):,} comparables\")\n",
    "print(f\"All property-subject pairs: {len(properties_df):,} pairs\")\n",
    "print(f\"Subjects in comps: {comps_df['subject_id'].nunique()}\")\n",
    "print(f\"Subjects in properties: {properties_df['subject_id'].nunique()}\")\n",
    "\n",
    "# Quick look at the data\n",
    "print(f\"\\nComps columns: {list(comps_df.columns[:5])}...\")\n",
    "print(f\"Properties columns: {list(properties_df.columns[:5])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Examine Data Structure and Key Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we're doing here:**\n",
    "- The `comps_df` contains the **real appraiser selections** - these are the actual properties that professional appraisers chose as the best comparables for each subject\n",
    "- The `properties_df` contains **all possible property-subject pairs** with our 90+ engineered features (distance, size similarity, composite scores, etc.)\n",
    "- We need to understand the structure of both datasets so we can map which properties in our engineered dataset were actually selected by appraisers\n",
    "\n",
    "**Why this matters:**\n",
    "- The appraiser selections become our **ground truth labels** (1 = selected, 0 = not selected)\n",
    "- This mapping is crucial for supervised learning - we're teaching the ML model to predict what appraisers would choose\n",
    "- We'll discover if our analytical scoring aligns with real appraiser preferences\n",
    "\n",
    "**What to look for:**\n",
    "- How many columns each dataset has and what they contain\n",
    "- Whether we have matching identifiers to link the datasets\n",
    "- Distribution of properties per subject (some subjects might have more candidates than others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç COMPS DATASET (Real Appraiser Selections):\n",
      "Shape: (264, 22)\n",
      "Columns: ['subject_id', 'comp_id', 'comp_index', 'condition', 'age_years', 'age_uncertainty', 'prop_type', 'city_province', 'address', 'lot_size_sqft', 'gla_sqft', 'bedrooms_main', 'bedrooms_additional', 'bedrooms_total', 'bathrooms_full', 'bathrooms_half', 'bathrooms_equivalent', 'sale_price', 'sale_date', 'has_lot_size_uncertainty', 'has_gla_uncertainty', 'has_additional_bedrooms']\n",
      "\n",
      "Sample comp data:\n",
      "   subject_id comp_id  comp_index  condition  age_years  age_uncertainty  \\\n",
      "0           0     0_0           0  Excellent       49.0            False   \n",
      "1           0     0_1           1       Fair       49.0            False   \n",
      "\n",
      "   prop_type        city_province             address  lot_size_sqft  ...  \\\n",
      "0  Townhouse  Kingston ON K7M 6V1  930 Amberdale Cres            NaN  ...   \n",
      "1  Townhouse  Kingston ON K7M 6X7      771 Ashwood Dr            NaN  ...   \n",
      "\n",
      "   bedrooms_additional  bedrooms_total  bathrooms_full  bathrooms_half  \\\n",
      "0                    0               3               2               0   \n",
      "1                    0               3               1               0   \n",
      "\n",
      "   bathrooms_equivalent  sale_price   sale_date  has_lot_size_uncertainty  \\\n",
      "0                   2.0    378900.0  2024-10-25                      True   \n",
      "1                   1.0    327000.0  2025-02-05                      True   \n",
      "\n",
      "  has_gla_uncertainty  has_additional_bedrooms  \n",
      "0               False                    False  \n",
      "1               False                    False  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "\n",
      "============================================================\n",
      "üîç PROPERTIES DATASET (All Candidates with Features):\n",
      "Shape: (7246, 92)\n",
      "Key columns: ['gla_sqft', 'close_price', 'gla_diff_sqft', 'gla_diff_pct', 'size_match_excellent', 'size_match_good', 'size_match_acceptable', 'bedroom_exact_match', 'bedroom_close_match', 'bathroom_exact_match', 'bathroom_close_match', 'structure_type_match', 'distance_km', 'distance_excellent', 'distance_good', 'distance_acceptable', 'recency_score', 'price_per_sqft', 'subject_price_per_sqft_est', 'price_per_sqft_diff', 'price_per_sqft_diff_pct', 'price_psf_very_similar', 'price_psf_similar', 'price_psf_acceptable', 'city_price_percentile', 'physical_score', 'location_score', 'temporal_score', 'market_score', 'composite_score']\n",
      "\n",
      "üìä Properties per subject distribution:\n",
      "Min: 3, Max: 723, Avg: 82.3\n"
     ]
    }
   ],
   "source": [
    "# Examine the structure of both datasets\n",
    "print(\"üîç COMPS DATASET (Real Appraiser Selections):\")\n",
    "print(f\"Shape: {comps_df.shape}\")\n",
    "print(f\"Columns: {list(comps_df.columns)}\")\n",
    "print(f\"\\nSample comp data:\")\n",
    "print(comps_df.head(2))\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üîç PROPERTIES DATASET (All Candidates with Features):\")\n",
    "print(f\"Shape: {properties_df.shape}\")\n",
    "print(f\"Key columns: {[col for col in properties_df.columns if any(x in col for x in ['score', 'match', 'distance', 'gla', 'price'])]}\")\n",
    "\n",
    "print(f\"\\nüìä Properties per subject distribution:\")\n",
    "props_per_subject = properties_df.groupby('subject_id').size()\n",
    "print(f\"Min: {props_per_subject.min()}, Max: {props_per_subject.max()}, Avg: {props_per_subject.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç INVESTIGATING ADDRESS MATCHING POTENTIAL\n",
      "============================================================\n",
      "üìä ADDRESS DATA ANALYSIS:\n",
      "   Comps with addresses: 264/264\n",
      "   Properties with addresses: 7246/7246\n",
      "\n",
      "üîç SAMPLE ADDRESS COMPARISON:\n",
      "   Comp addresses:\n",
      "     '930 Amberdale Cres'\n",
      "     '771 Ashwood Dr'\n",
      "     '995 Amberdale Cres'\n",
      "     '64 Deermist Dr'\n",
      "     '85 Oceanic Dr'\n",
      "   Property addresses:\n",
      "     '692 Truedell Rd'\n",
      "     '1034 Craig Lane '\n",
      "     '950 Oakview Avenue '\n",
      "     'Unit 51 - 808 Datzell Lane'\n",
      "     '771 ASHWOOD Dr '\n",
      "\n",
      "üîç SUBJECT 0 ADDRESS ANALYSIS:\n",
      "   Comp addresses (3):\n",
      "     '930 Amberdale Cres'\n",
      "     '771 Ashwood Dr'\n",
      "     '995 Amberdale Cres'\n",
      "   Property addresses (first 5 of 121):\n",
      "     '692 Truedell Rd'\n",
      "     '1034 Craig Lane '\n",
      "     '950 Oakview Avenue '\n",
      "     'Unit 51 - 808 Datzell Lane'\n",
      "     '771 ASHWOOD Dr '\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Investigate Address Matching Issues and Create Hybrid Mapping\n",
    "print(\"üîç INVESTIGATING ADDRESS MATCHING POTENTIAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# First, let's see what the address data looks like\n",
    "print(\"üìä ADDRESS DATA ANALYSIS:\")\n",
    "print(f\"   Comps with addresses: {comps_df['address'].notna().sum()}/{len(comps_df)}\")\n",
    "print(f\"   Properties with addresses: {properties_df['address'].notna().sum()}/{len(properties_df)}\")\n",
    "\n",
    "# Sample addresses from both datasets\n",
    "print(f\"\\nüîç SAMPLE ADDRESS COMPARISON:\")\n",
    "print(\"   Comp addresses:\")\n",
    "for addr in comps_df['address'].dropna().head(5):\n",
    "    print(f\"     '{addr}'\")\n",
    "\n",
    "print(\"   Property addresses:\")  \n",
    "for addr in properties_df['address'].dropna().head(5):\n",
    "    print(f\"     '{addr}'\")\n",
    "\n",
    "# Test address matching for Subject 0 to see the issue\n",
    "subject_0_comps = comps_df[comps_df['subject_id'] == 0]\n",
    "subject_0_props = properties_df[properties_df['subject_id'] == 0]\n",
    "\n",
    "print(f\"\\nüîç SUBJECT 0 ADDRESS ANALYSIS:\")\n",
    "print(f\"   Comp addresses ({len(subject_0_comps)}):\")\n",
    "for _, comp in subject_0_comps.iterrows():\n",
    "    print(f\"     '{comp['address']}'\")\n",
    "\n",
    "print(f\"   Property addresses (first 5 of {len(subject_0_props)}):\")\n",
    "for _, prop in subject_0_props.head(5).iterrows():\n",
    "    print(f\"     '{prop['address']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß TESTING IMPROVED ADDRESS MATCHING\n",
      "==================================================\n",
      "üß™ TESTING ON SUBJECT 0:\n",
      "   Comp: '771 Ashwood Dr' ‚Üí '771 ASHWOOD DR'\n",
      "   Prop: '771 ASHWOOD Dr ' ‚Üí '771 ASHWOOD DR'\n",
      "   Similarity: 1.000\n",
      "\n",
      "üìç SUBJECT 0 FULL ADDRESS MATCHING TEST:\n",
      "\n",
      "   Comp: '930 Amberdale Cres'\n",
      "   Best matches:\n",
      "     0.889: '995 Amberdale Cres ' (ID: 64)\n",
      "     0.722: '941 AMBLESIDE Crescent ' (ID: 120)\n",
      "     0.649: 'Unit 101 - 1010 Pembridge Crescent ' (ID: 72)\n",
      "\n",
      "   Comp: '771 Ashwood Dr'\n",
      "   Best matches:\n",
      "     1.000: '771 ASHWOOD Dr ' (ID: 47)\n",
      "     0.710: '371 Tanglewood Drive ' (ID: 143)\n",
      "     0.688: '871 Larchwood Crescent ' (ID: 92)\n",
      "\n",
      "   Comp: '995 Amberdale Cres'\n",
      "   Best matches:\n",
      "     1.000: '995 Amberdale Cres ' (ID: 64)\n",
      "     0.722: '941 AMBLESIDE Crescent ' (ID: 120)\n",
      "     0.667: '995 Waterbury Crescent ' (ID: 70)\n"
     ]
    }
   ],
   "source": [
    "# Test improved address normalization and matching\n",
    "print(\"üîß TESTING IMPROVED ADDRESS MATCHING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def normalize_address_improved(address):\n",
    "    \"\"\"Improved address normalization\"\"\"\n",
    "    if pd.isna(address):\n",
    "        return \"\"\n",
    "    \n",
    "    addr = str(address).upper().strip()\n",
    "    \n",
    "    # Remove common prefixes/suffixes\n",
    "    addr = re.sub(r'^UNIT\\s+\\d+\\s*-\\s*', '', addr)  # Remove \"Unit 51 - \"\n",
    "    addr = re.sub(r'^\\d+\\s*-\\s*', '', addr)  # Remove \"51 - \"\n",
    "    \n",
    "    # Common abbreviation mappings\n",
    "    abbreviations = {\n",
    "        'STREET': 'ST', 'AVENUE': 'AVE', 'ROAD': 'RD', 'DRIVE': 'DR',\n",
    "        'CRESCENT': 'CRES', 'BOULEVARD': 'BLVD', 'PLACE': 'PL',\n",
    "        'COURT': 'CT', 'LANE': 'LN', 'CIRCLE': 'CIR', 'TERRACE': 'TER'\n",
    "    }\n",
    "    \n",
    "    for full, abbrev in abbreviations.items():\n",
    "        addr = addr.replace(full, abbrev)\n",
    "    \n",
    "    # Remove extra spaces and punctuation\n",
    "    addr = re.sub(r'[^\\w\\s]', '', addr)\n",
    "    addr = re.sub(r'\\s+', ' ', addr)\n",
    "    \n",
    "    return addr.strip()\n",
    "\n",
    "def address_similarity_improved(addr1, addr2):\n",
    "    \"\"\"Calculate similarity between two addresses\"\"\"\n",
    "    norm1 = normalize_address_improved(addr1)\n",
    "    norm2 = normalize_address_improved(addr2)\n",
    "    \n",
    "    if not norm1 or not norm2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Exact match gets full score\n",
    "    if norm1 == norm2:\n",
    "        return 1.0\n",
    "    \n",
    "    # Use sequence matcher for partial similarity\n",
    "    return SequenceMatcher(None, norm1, norm2).ratio()\n",
    "\n",
    "# Test the improved normalization on Subject 0\n",
    "print(\"üß™ TESTING ON SUBJECT 0:\")\n",
    "comp_addr = \"771 Ashwood Dr\"\n",
    "prop_addr = \"771 ASHWOOD Dr \"\n",
    "\n",
    "print(f\"   Comp: '{comp_addr}' ‚Üí '{normalize_address_improved(comp_addr)}'\")\n",
    "print(f\"   Prop: '{prop_addr}' ‚Üí '{normalize_address_improved(prop_addr)}'\")\n",
    "print(f\"   Similarity: {address_similarity_improved(comp_addr, prop_addr):.3f}\")\n",
    "\n",
    "# Test on all Subject 0 addresses\n",
    "print(f\"\\nüìç SUBJECT 0 FULL ADDRESS MATCHING TEST:\")\n",
    "for _, comp in subject_0_comps.iterrows():\n",
    "    comp_addr = comp['address']\n",
    "    print(f\"\\n   Comp: '{comp_addr}'\")\n",
    "    \n",
    "    best_matches = []\n",
    "    for _, prop in subject_0_props.iterrows():\n",
    "        prop_addr = prop['address']\n",
    "        similarity = address_similarity_improved(comp_addr, prop_addr)\n",
    "        if similarity > 0.5:  # Only show decent matches\n",
    "            best_matches.append((prop_addr, similarity, prop['property_id']))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    best_matches.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if best_matches:\n",
    "        print(f\"   Best matches:\")\n",
    "        for prop_addr, sim, prop_id in best_matches[:3]:\n",
    "            print(f\"     {sim:.3f}: '{prop_addr}' (ID: {prop_id})\")\n",
    "    else:\n",
    "        print(f\"   No good address matches found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ CREATING FINAL HYBRID MAPPING (CHARACTERISTICS + ADDRESS)\n",
      "======================================================================\n",
      "üîÑ Processing all 264 appraiser selections...\n",
      "\n",
      "‚úÖ FINAL HYBRID MAPPING RESULTS:\n",
      "   Total appraiser selections: 264\n",
      "   Successfully mapped: 219\n",
      "   Mapping rate: 83.0%\n",
      "   Unmatched: 0\n",
      "\n",
      "üìä SCORE ANALYSIS:\n",
      "   Average total score: 0.793\n",
      "   Average char score: 0.588\n",
      "   Average addr score: 0.204\n",
      "   Average addr similarity: 0.682\n",
      "   Score range: 0.434 - 1.000\n",
      "\n",
      "üéØ QUALITY DISTRIBUTION:\n",
      "   High quality (‚â•0.7): 199 (75.4%)\n",
      "   Medium quality (0.5-0.7): 59 (22.3%)\n",
      "   Lower quality (0.4-0.5): 6 (2.3%)\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Create Final Hybrid Mapping\n",
    "print(\"üéØ CREATING FINAL HYBRID MAPPING (CHARACTERISTICS + ADDRESS)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def find_best_matching_property_hybrid(comp_row, subject_properties):\n",
    "    \"\"\"Find best matching property using characteristics + address\"\"\"\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    best_breakdown = {}\n",
    "    \n",
    "    for _, prop in subject_properties.iterrows():\n",
    "        # CHARACTERISTICS SIMILARITY (70% weight)\n",
    "        char_score = 0\n",
    "        \n",
    "        # GLA similarity (25% of total = 35% of char score)\n",
    "        if pd.notna(comp_row['gla_sqft']) and pd.notna(prop['gla_sqft']):\n",
    "            if comp_row['gla_sqft'] == prop['gla_sqft']:\n",
    "                char_score += 0.25\n",
    "            else:\n",
    "                gla_diff = abs(comp_row['gla_sqft'] - prop['gla_sqft']) / max(comp_row['gla_sqft'], prop['gla_sqft'])\n",
    "                char_score += max(0, 0.25 * (1 - gla_diff))\n",
    "        \n",
    "        # Price similarity (25% of total)\n",
    "        if pd.notna(comp_row['sale_price']) and pd.notna(prop['close_price']):\n",
    "            if comp_row['sale_price'] == prop['close_price']:\n",
    "                char_score += 0.25\n",
    "            else:\n",
    "                price_diff = abs(comp_row['sale_price'] - prop['close_price']) / max(comp_row['sale_price'], prop['close_price'])\n",
    "                char_score += max(0, 0.25 * (1 - price_diff))\n",
    "        \n",
    "        # Bedroom similarity (10% of total)\n",
    "        if pd.notna(comp_row['bedrooms_total']) and pd.notna(prop['bedrooms_total']):\n",
    "            if comp_row['bedrooms_total'] == prop['bedrooms_total']:\n",
    "                char_score += 0.10\n",
    "        \n",
    "        # Structure type similarity (10% of total)\n",
    "        if pd.notna(comp_row['prop_type']) and pd.notna(prop['structure_type']):\n",
    "            if comp_row['prop_type'].lower() in prop['structure_type'].lower():\n",
    "                char_score += 0.10\n",
    "        \n",
    "        # ADDRESS SIMILARITY (30% weight)\n",
    "        addr_similarity = address_similarity_improved(comp_row.get('address', ''), prop.get('address', ''))\n",
    "        addr_score = 0.30 * addr_similarity\n",
    "        \n",
    "        # TOTAL SCORE\n",
    "        total_score = char_score + addr_score\n",
    "        \n",
    "        if total_score > best_score:\n",
    "            best_score = total_score\n",
    "            best_match = prop['property_id']\n",
    "            best_breakdown = {\n",
    "                'char_score': char_score,\n",
    "                'addr_score': addr_score,\n",
    "                'addr_similarity': addr_similarity,\n",
    "                'total_score': total_score\n",
    "            }\n",
    "    \n",
    "    return best_match, best_score, best_breakdown\n",
    "\n",
    "# Create final hybrid mapping\n",
    "print(\"üîÑ Processing all 264 appraiser selections...\")\n",
    "appraiser_selections = set()\n",
    "mapping_details = []\n",
    "unmatched_count = 0\n",
    "\n",
    "for _, comp in comps_df.iterrows():\n",
    "    subject_id = comp['subject_id']\n",
    "    subject_properties = properties_df[properties_df['subject_id'] == subject_id]\n",
    "    \n",
    "    if len(subject_properties) > 0:\n",
    "        matched_property_id, match_score, breakdown = find_best_matching_property_hybrid(comp, subject_properties)\n",
    "        \n",
    "        # Accept matches with score >= 0.4 (balanced threshold)\n",
    "        if matched_property_id and match_score >= 0.4:\n",
    "            appraiser_selections.add((subject_id, matched_property_id))\n",
    "            mapping_details.append({\n",
    "                'subject_id': subject_id,\n",
    "                'comp_id': comp['comp_id'],\n",
    "                'property_id': matched_property_id,\n",
    "                'total_score': match_score,\n",
    "                'char_score': breakdown['char_score'],\n",
    "                'addr_score': breakdown['addr_score'],\n",
    "                'addr_similarity': breakdown['addr_similarity']\n",
    "            })\n",
    "        else:\n",
    "            unmatched_count += 1\n",
    "\n",
    "# Results summary\n",
    "print(f\"\\n‚úÖ FINAL HYBRID MAPPING RESULTS:\")\n",
    "print(f\"   Total appraiser selections: 264\")\n",
    "print(f\"   Successfully mapped: {len(appraiser_selections)}\")\n",
    "print(f\"   Mapping rate: {len(appraiser_selections)/264:.1%}\")\n",
    "print(f\"   Unmatched: {unmatched_count}\")\n",
    "\n",
    "if mapping_details:\n",
    "    total_scores = [m['total_score'] for m in mapping_details]\n",
    "    char_scores = [m['char_score'] for m in mapping_details]\n",
    "    addr_scores = [m['addr_score'] for m in mapping_details]\n",
    "    addr_similarities = [m['addr_similarity'] for m in mapping_details]\n",
    "    \n",
    "    print(f\"\\nüìä SCORE ANALYSIS:\")\n",
    "    print(f\"   Average total score: {np.mean(total_scores):.3f}\")\n",
    "    print(f\"   Average char score: {np.mean(char_scores):.3f}\")\n",
    "    print(f\"   Average addr score: {np.mean(addr_scores):.3f}\")\n",
    "    print(f\"   Average addr similarity: {np.mean(addr_similarities):.3f}\")\n",
    "    print(f\"   Score range: {min(total_scores):.3f} - {max(total_scores):.3f}\")\n",
    "    \n",
    "    # Quality distribution\n",
    "    high_quality = sum(1 for s in total_scores if s >= 0.7)\n",
    "    medium_quality = sum(1 for s in total_scores if 0.5 <= s < 0.7)\n",
    "    lower_quality = sum(1 for s in total_scores if 0.4 <= s < 0.5)\n",
    "    \n",
    "    print(f\"\\nüéØ QUALITY DISTRIBUTION:\")\n",
    "    print(f\"   High quality (‚â•0.7): {high_quality} ({high_quality/len(mapping_details):.1%})\")\n",
    "    print(f\"   Medium quality (0.5-0.7): {medium_quality} ({medium_quality/len(mapping_details):.1%})\")\n",
    "    print(f\"   Lower quality (0.4-0.5): {lower_quality} ({lower_quality/len(mapping_details):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Binary Labels for ML Training\n",
    "\n",
    "**What we're doing:**\n",
    "- Add `is_appraiser_selected` column to our properties dataset \n",
    "- Mark properties that real appraisers selected as 1, all others as 0\n",
    "- This creates our supervised learning target variable for training\n",
    "\n",
    "**Why this matters:**\n",
    "- The binary labels become our **ground truth** for ML training\n",
    "- 1 = \"A professional appraiser chose this property as a comparable\"\n",
    "- 0 = \"This property was available but the appraiser didn't choose it\"\n",
    "- We can now train ML models to predict appraiser preferences\n",
    "\n",
    "**Expected outcome:**\n",
    "- Highly imbalanced dataset (~3% positive class) - this is normal!\n",
    "- Real appraisers are very selective (only ~3 out of 80+ properties per subject)\n",
    "- We'll need special techniques to handle this imbalance in ML training\n",
    "- This gives us the foundation for supervised learning\n",
    "\n",
    "**What this enables:**\n",
    "- Train models to learn: \"What makes appraisers choose certain properties?\"\n",
    "- Compare our analytical scoring vs real appraiser preferences\n",
    "- Build ML models that can predict appraiser-quality recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè∑Ô∏è CREATING BINARY LABELS FOR ML TRAINING\n",
      "============================================================\n",
      "üìä CLASS DISTRIBUTION ANALYSIS:\n",
      "   Total property-subject pairs: 7,246\n",
      "   Appraiser-selected properties: 219\n",
      "   Not selected properties: 7,027\n",
      "   Selection rate: 3.0%\n",
      "\n",
      "üéØ SUBJECT COVERAGE:\n",
      "   Total subjects: 88\n",
      "   Subjects with mapped selections: 88\n",
      "   Coverage rate: 100.0%\n",
      "\n",
      "üìà SELECTIONS PER SUBJECT:\n",
      "   Min: 1\n",
      "   Max: 3\n",
      "   Average: 2.5\n",
      "   Median: 3.0\n",
      "\n",
      "‚öñÔ∏è CLASS IMBALANCE ANALYSIS:\n",
      "   ‚ö†Ô∏è  Highly imbalanced dataset (3.0% positive class)\n",
      "   üí° Will need to use techniques like:\n",
      "      - Class weights in ML models\n",
      "      - Stratified sampling\n",
      "      - Precision-Recall metrics instead of accuracy\n",
      "\n",
      "üîç SAMPLE LABELED DATA:\n",
      "   subject_id  property_id  composite_score  is_appraiser_selected\n",
      "0           0           28             79.5                      0\n",
      "1           0           96             78.0                      0\n",
      "2           0          125             75.5                      0\n",
      "3           0           32             75.5                      0\n",
      "4           0           47             74.0                      1\n",
      "5           0           64             74.0                      1\n",
      "6           0           52             71.0                      0\n",
      "7           0            4             70.0                      0\n",
      "8           0           26             70.0                      0\n",
      "9           0           40             70.0                      0\n",
      "\n",
      "‚úÖ BINARY LABELS CREATED SUCCESSFULLY!\n",
      "   Dataset ready for ML training with 7,246 samples\n",
      "   Target variable: 'is_appraiser_selected' (0/1)\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Create Binary Labels for ML Training\n",
    "print(\"üè∑Ô∏è CREATING BINARY LABELS FOR ML TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create the binary label column\n",
    "properties_df['is_appraiser_selected'] = 0\n",
    "\n",
    "# Mark the appraiser-selected properties\n",
    "for subject_id, property_id in appraiser_selections:\n",
    "    mask = (properties_df['subject_id'] == subject_id) & (properties_df['property_id'] == property_id)\n",
    "    properties_df.loc[mask, 'is_appraiser_selected'] = 1\n",
    "\n",
    "# Analyze the class distribution\n",
    "total_properties = len(properties_df)\n",
    "selected_properties = properties_df['is_appraiser_selected'].sum()\n",
    "selection_rate = selected_properties / total_properties\n",
    "\n",
    "print(f\"üìä CLASS DISTRIBUTION ANALYSIS:\")\n",
    "print(f\"   Total property-subject pairs: {total_properties:,}\")\n",
    "print(f\"   Appraiser-selected properties: {selected_properties:,}\")\n",
    "print(f\"   Not selected properties: {total_properties - selected_properties:,}\")\n",
    "print(f\"   Selection rate: {selection_rate:.1%}\")\n",
    "\n",
    "# Analyze by subject\n",
    "subjects_with_selections = properties_df[properties_df['is_appraiser_selected'] == 1]['subject_id'].nunique()\n",
    "total_subjects = properties_df['subject_id'].nunique()\n",
    "\n",
    "print(f\"\\nüéØ SUBJECT COVERAGE:\")\n",
    "print(f\"   Total subjects: {total_subjects}\")\n",
    "print(f\"   Subjects with mapped selections: {subjects_with_selections}\")\n",
    "print(f\"   Coverage rate: {subjects_with_selections/total_subjects:.1%}\")\n",
    "\n",
    "# Show selection distribution per subject\n",
    "selections_per_subject = properties_df[properties_df['is_appraiser_selected'] == 1].groupby('subject_id').size()\n",
    "print(f\"\\nüìà SELECTIONS PER SUBJECT:\")\n",
    "print(f\"   Min: {selections_per_subject.min()}\")\n",
    "print(f\"   Max: {selections_per_subject.max()}\")\n",
    "print(f\"   Average: {selections_per_subject.mean():.1f}\")\n",
    "print(f\"   Median: {selections_per_subject.median():.1f}\")\n",
    "\n",
    "# Check for class imbalance (expected to be highly imbalanced)\n",
    "print(f\"\\n‚öñÔ∏è CLASS IMBALANCE ANALYSIS:\")\n",
    "if selection_rate < 0.1:\n",
    "    print(f\"   ‚ö†Ô∏è  Highly imbalanced dataset ({selection_rate:.1%} positive class)\")\n",
    "    print(f\"   üí° Will need to use techniques like:\")\n",
    "    print(f\"      - Class weights in ML models\")\n",
    "    print(f\"      - Stratified sampling\")\n",
    "    print(f\"      - Precision-Recall metrics instead of accuracy\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Reasonably balanced dataset\")\n",
    "\n",
    "# Sample of the labeled data\n",
    "print(f\"\\nüîç SAMPLE LABELED DATA:\")\n",
    "sample_data = properties_df[['subject_id', 'property_id', 'composite_score', 'is_appraiser_selected']].head(10)\n",
    "print(sample_data)\n",
    "\n",
    "print(f\"\\n‚úÖ BINARY LABELS CREATED SUCCESSFULLY!\")\n",
    "print(f\"   Dataset ready for ML training with {len(properties_df):,} samples\")\n",
    "print(f\"   Target variable: 'is_appraiser_selected' (0/1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Prepare Base Features for ML Training\n",
    "\n",
    "**Strategy: Use Raw Features Only**\n",
    "- **Problem**: Our current 24+ engineered features include pre-computed similarity scores and ratios\n",
    "- **Issue**: This constrains the ML model and adds noise - the model should learn patterns itself\n",
    "- **Solution**: Use only raw/base features and let ML algorithms discover optimal patterns\n",
    "\n",
    "**Base Feature Categories (12-15 features):**\n",
    "- **Physical**: square_feet, bedrooms, bathrooms, structure_type \n",
    "- **Location**: latitude, longitude, distance_km, same_city\n",
    "- **Temporal**: sale_date (as numeric), days_since_sale\n",
    "- **Market**: sale_price, price_per_sqft\n",
    "- **Subject Reference**: subject_square_feet, subject_bedrooms, subject_bathrooms, subject_latitude, subject_longitude\n",
    "\n",
    "**Why This Approach:**\n",
    "- ML excels at finding complex patterns in raw data\n",
    "- Engineered similarity scores constrain learning\n",
    "- Simpler feature space reduces overfitting  \n",
    "- Model learns optimal feature interactions and weightings\n",
    "- More generalizable to new data\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Clean, minimal feature matrix (~12-15 features)\n",
    "- Raw data that lets ML algorithms shine\n",
    "- Better generalization and interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó ADDING KEY SUBJECT FEATURES\n",
      "==================================================\n",
      "üìä SUBJECT FEATURES TO ADD:\n",
      "   Features: ['subject_id', 'subject_condition', 'subject_age_years', 'subject_effective_date', 'subject_structure_type', 'subject_gla_sqft', 'subject_bedrooms', 'subject_municipality_district', 'subject_bathrooms']\n",
      "   Sample data:\n",
      "   subject_id subject_condition  subject_age_years subject_effective_date  \\\n",
      "0           0           Average                NaN            Apr/11/2025   \n",
      "1           1           Average                NaN            Apr/17/2025   \n",
      "2           2           Average                NaN            May/01/2025   \n",
      "\n",
      "  subject_structure_type  subject_gla_sqft  subject_bedrooms  \\\n",
      "0              Townhouse            1044.0               3.0   \n",
      "1               Detached            1500.0               3.0   \n",
      "2               Detached            3000.0               4.0   \n",
      "\n",
      "                      subject_municipality_district  subject_bathrooms  \n",
      "0                                          Kingston                1.5  \n",
      "1  Halifax Regional Municipality - West Chezzetcook                2.5  \n",
      "2    Township of North Dumfries, Region of Waterloo                2.5  \n",
      "\n",
      "üîó MERGE RESULTS:\n",
      "   Before: (7246, 93)\n",
      "   After: (7246, 101)\n",
      "   Success: 100.0%\n",
      "\n",
      "‚úÖ SUBJECT FEATURES ADDED!\n",
      "   New columns added: 8\n",
      "   Ready for ML training with subject context\n"
     ]
    }
   ],
   "source": [
    "# STEP 8.5: ADD KEY SUBJECT FEATURES\n",
    "print(\"üîó ADDING KEY SUBJECT FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Parse bathrooms from string format (e.g., \"2:1\" -> 2.5)\n",
    "def parse_bathrooms(bath_str):\n",
    "    \"\"\"Convert bathroom string like '2:1' to numeric (2.5)\"\"\"\n",
    "    if pd.isna(bath_str) or bath_str == '':\n",
    "        return np.nan\n",
    "    \n",
    "    bath_str = str(bath_str).strip()\n",
    "    \n",
    "    # Handle special formats\n",
    "    if 'F' in bath_str or 'H' in bath_str:\n",
    "        # Format like \"2F 1H\" or \"3F\"\n",
    "        full = len([x for x in bath_str.split() if 'F' in x])\n",
    "        half = len([x for x in bath_str.split() if 'H' in x])\n",
    "        return full + (half * 0.5)\n",
    "    \n",
    "    if ':' in bath_str:\n",
    "        # Format like \"2:1\" \n",
    "        parts = bath_str.split(':')\n",
    "        if len(parts) == 2:\n",
    "            full = float(parts[0]) if parts[0].isdigit() else 0\n",
    "            half = float(parts[1]) if parts[1].isdigit() else 0\n",
    "            return full + (half * 0.5)\n",
    "    \n",
    "    # Try direct conversion\n",
    "    try:\n",
    "        return float(bath_str)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Select and clean subject features\n",
    "subject_features = subject_df[['subject_id', 'condition', 'age_years', 'effective_date', \n",
    "                              'structure_type', 'gla_sqft', 'bedrooms_raw', 'bathrooms_raw', \n",
    "                              'municipality_district']].copy()\n",
    "\n",
    "# Rename columns to have subject_ prefix for clarity\n",
    "subject_features.rename(columns={\n",
    "    'condition': 'subject_condition',\n",
    "    'age_years': 'subject_age_years', \n",
    "    'effective_date': 'subject_effective_date',\n",
    "    'structure_type': 'subject_structure_type',\n",
    "    'gla_sqft': 'subject_gla_sqft',\n",
    "    'bedrooms_raw': 'subject_bedrooms',\n",
    "    'bathrooms_raw': 'subject_bathrooms_raw',\n",
    "    'municipality_district': 'subject_municipality_district'\n",
    "}, inplace=True)\n",
    "\n",
    "# Clean the bathrooms field\n",
    "subject_features['subject_bathrooms'] = subject_features['subject_bathrooms_raw'].apply(parse_bathrooms)\n",
    "subject_features['subject_bedrooms'] = pd.to_numeric(subject_features['subject_bedrooms'], errors='coerce')\n",
    "subject_features['subject_gla_sqft'] = pd.to_numeric(subject_features['subject_gla_sqft'], errors='coerce')\n",
    "\n",
    "# Drop the raw bathrooms column\n",
    "subject_features.drop('subject_bathrooms_raw', axis=1, inplace=True)\n",
    "\n",
    "print(f\"üìä SUBJECT FEATURES TO ADD:\")\n",
    "print(f\"   Features: {list(subject_features.columns)}\")\n",
    "print(f\"   Sample data:\")\n",
    "print(subject_features.head(3))\n",
    "\n",
    "# Merge with properties data\n",
    "properties_with_subjects = properties_df.merge(subject_features, on='subject_id', how='left')\n",
    "\n",
    "print(f\"\\nüîó MERGE RESULTS:\")\n",
    "print(f\"   Before: {properties_df.shape}\")\n",
    "print(f\"   After: {properties_with_subjects.shape}\")\n",
    "print(f\"   Success: {(~properties_with_subjects['subject_gla_sqft'].isna()).mean():.1%}\")\n",
    "\n",
    "# Update main dataframe\n",
    "properties_df = properties_with_subjects.copy()\n",
    "\n",
    "print(f\"\\n‚úÖ SUBJECT FEATURES ADDED!\")\n",
    "print(f\"   New columns added: {len(subject_features.columns)-1}\")  # -1 for subject_id\n",
    "print(f\"   Ready for ML training with subject context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß USING CORRECTED BASE FEATURES\n",
      "==================================================\n",
      "Corrected features (19):\n",
      "    1. gla_sqft\n",
      "    2. bedrooms_total\n",
      "    3. bathrooms_equivalent\n",
      "    4. close_price\n",
      "    5. price_per_sqft\n",
      "    6. latitude\n",
      "    7. longitude\n",
      "    8. distance_km\n",
      "    9. same_city\n",
      "   10. close_date\n",
      "   11. days_from_effective\n",
      "   12. prop_type_clean\n",
      "   13. structure_type_match\n",
      "   14. subject_condition\n",
      "   15. subject_structure_type\n",
      "   16. subject_gla_sqft\n",
      "   17. subject_bedrooms\n",
      "   18. subject_bathrooms\n",
      "   19. subject_municipality_district\n",
      "\n",
      "Available: 19/19\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED: Use the base features that actually exist\n",
    "corrected_base_features = [\n",
    "    # Physical characteristics (raw)\n",
    "    'gla_sqft',                    # square footage\n",
    "    'bedrooms_total',              # bedrooms  \n",
    "    'bathrooms_equivalent',        # bathrooms\n",
    "    'close_price',                 # sale price\n",
    "    'price_per_sqft',             # price per sqft\n",
    "    \n",
    "    # Location (raw)\n",
    "    'latitude', 'longitude',       # coordinates\n",
    "    'distance_km',                 # distance to subject\n",
    "    'same_city',                   # same city indicator\n",
    "    \n",
    "    # Temporal (raw)\n",
    "    'close_date',                  # sale date\n",
    "    'days_from_effective',         # days since sale\n",
    "    \n",
    "    # Categorical (raw)\n",
    "    'prop_type_clean',             # property type\n",
    "    'structure_type_match',        # structure match\n",
    "    \n",
    "    # Subject features\n",
    "    'subject_condition',           # subject condition          # subject age\n",
    "    'subject_structure_type',     # subject structure type\n",
    "    'subject_gla_sqft',          # subject square footage\n",
    "    'subject_bedrooms',          # subject bedrooms\n",
    "    'subject_bathrooms',         # subject bathrooms\n",
    "    'subject_municipality_district' # subject location\n",
    "]\n",
    "\n",
    "print(\"üîß USING CORRECTED BASE FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Corrected features ({len(corrected_base_features)}):\")\n",
    "for i, feat in enumerate(corrected_base_features, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")\n",
    "\n",
    "# Check availability\n",
    "available_corrected = [f for f in corrected_base_features if f in properties_df.columns]\n",
    "missing_corrected = [f for f in corrected_base_features if f not in properties_df.columns]\n",
    "\n",
    "print(f\"\\nAvailable: {len(available_corrected)}/{len(corrected_base_features)}\")\n",
    "if missing_corrected:\n",
    "    print(f\"Still missing: {missing_corrected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CREATING CLEAN FEATURE MATRIX FOR ML TRAINING\n",
      "============================================================\n",
      "üîç CHECKING FEATURE AVAILABILITY:\n",
      "   Available features (19): ['gla_sqft', 'bedrooms_total', 'bathrooms_equivalent', 'close_price', 'price_per_sqft', 'latitude', 'longitude', 'distance_km', 'same_city', 'close_date', 'days_from_effective', 'prop_type_clean', 'structure_type_match', 'subject_condition', 'subject_structure_type', 'subject_gla_sqft', 'subject_bedrooms', 'subject_bathrooms', 'subject_municipality_district']\n",
      "\n",
      "üìä INITIAL FEATURE MATRIX:\n",
      "   Shape: (7246, 19)\n",
      "   Features: 19\n",
      "   Samples: 7,246\n",
      "\n",
      "üìÖ PROCESSING TEMPORAL FEATURES:\n",
      "   ‚úÖ Converted close_date to close_date_numeric (days since epoch)\n",
      "\n",
      "üîß ENCODING CATEGORICAL FEATURES:\n",
      "   prop_type_clean: 9 unique values\n",
      "     ‚Üí Encoded prop_type_clean with LabelEncoder\n",
      "   subject_condition: 4 unique values\n",
      "     ‚Üí Encoded subject_condition with LabelEncoder\n",
      "   subject_structure_type: 9 unique values\n",
      "     ‚Üí Encoded subject_structure_type with LabelEncoder\n",
      "   subject_municipality_district: 74 unique values\n",
      "     ‚Üí Encoded subject_municipality_district with LabelEncoder\n",
      "\n",
      "üîß HANDLING MISSING VALUES:\n",
      "   Features with missing values:\n",
      "     gla_sqft: 130 (0.0%)\n",
      "     close_price: 36 (0.0%)\n",
      "     price_per_sqft: 156 (0.0%)\n",
      "     subject_bedrooms: 61 (0.0%)\n",
      "     subject_bathrooms: 32 (0.0%)\n",
      "     ‚Üí Filled gla_sqft missing values with median: 1300.00\n",
      "     ‚Üí Filled close_price missing values with median: 605000.00\n",
      "     ‚Üí Filled price_per_sqft missing values with median: 434.53\n",
      "     ‚Üí Filled subject_bedrooms missing values with median: 3.00\n",
      "     ‚Üí Filled subject_bathrooms missing values with median: 2.00\n",
      "   ‚úÖ Applied median imputation\n",
      "   Remaining missing values: 0\n",
      "\n",
      "üìã FINAL FEATURE MATRIX:\n",
      "   Shape: (7246, 19)\n",
      "   Features: ['gla_sqft', 'bedrooms_total', 'bathrooms_equivalent', 'close_price', 'price_per_sqft', 'latitude', 'longitude', 'distance_km', 'same_city', 'days_from_effective', 'prop_type_clean', 'structure_type_match', 'subject_condition', 'subject_structure_type', 'subject_gla_sqft', 'subject_bedrooms', 'subject_bathrooms', 'subject_municipality_district', 'close_date_numeric']\n",
      "   All features are numerical: True\n",
      "   Target distribution: 219 positive (3.0%), -7465 negative\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: Create Clean Feature Matrix and Train/Test Split\n",
    "print(\"üîß CREATING CLEAN FEATURE MATRIX FOR ML TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# First, check which features actually exist\n",
    "print(\"üîç CHECKING FEATURE AVAILABILITY:\")\n",
    "available_features = []\n",
    "missing_features = []\n",
    "\n",
    "for feat in corrected_base_features:\n",
    "    if feat in properties_df.columns:\n",
    "        available_features.append(feat)\n",
    "    else:\n",
    "        missing_features.append(feat)\n",
    "\n",
    "print(f\"   Available features ({len(available_features)}): {available_features}\")\n",
    "if missing_features:\n",
    "    print(f\"   Missing features ({len(missing_features)}): {missing_features}\")\n",
    "\n",
    "# Use only available features\n",
    "X_clean = properties_df[available_features].copy()\n",
    "y = properties_df['is_appraiser_selected'].copy()\n",
    "subjects = properties_df['subject_id'].copy()\n",
    "\n",
    "print(f\"\\nüìä INITIAL FEATURE MATRIX:\")\n",
    "print(f\"   Shape: {X_clean.shape}\")\n",
    "print(f\"   Features: {len(available_features)}\")\n",
    "print(f\"   Samples: {len(X_clean):,}\")\n",
    "\n",
    "# Handle temporal features FIRST - convert close_date to numerical\n",
    "print(f\"\\nüìÖ PROCESSING TEMPORAL FEATURES:\")\n",
    "if 'close_date' in X_clean.columns:\n",
    "    # Convert to datetime and then to numerical (days since epoch)\n",
    "    X_clean['close_date'] = pd.to_datetime(X_clean['close_date'])\n",
    "    X_clean['close_date_numeric'] = (X_clean['close_date'] - pd.Timestamp('1970-01-01')).dt.days\n",
    "    \n",
    "    # Drop original date column, keep numeric version\n",
    "    X_clean = X_clean.drop('close_date', axis=1)\n",
    "    print(f\"   ‚úÖ Converted close_date to close_date_numeric (days since epoch)\")\n",
    "\n",
    "# Handle subject_effective_date if it exists\n",
    "if 'subject_effective_date' in X_clean.columns:\n",
    "    # For now, drop it since it's a date string that's hard to parse\n",
    "    X_clean = X_clean.drop('subject_effective_date', axis=1)\n",
    "    print(f\"   ‚úÖ Dropped subject_effective_date (date string)\")\n",
    "\n",
    "# Handle categorical features - encode for ML\n",
    "print(f\"\\nüîß ENCODING CATEGORICAL FEATURES:\")\n",
    "categorical_cols = X_clean.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    unique_vals = X_clean[col].nunique()\n",
    "    print(f\"   {col}: {unique_vals} unique values\")\n",
    "    \n",
    "    # Use LabelEncoder for all categorical features\n",
    "    le = LabelEncoder()\n",
    "    X_clean[col] = le.fit_transform(X_clean[col].astype(str))\n",
    "    print(f\"     ‚Üí Encoded {col} with LabelEncoder\")\n",
    "\n",
    "# Handle missing values using a simpler approach\n",
    "print(f\"\\nüîß HANDLING MISSING VALUES:\")\n",
    "missing_summary = X_clean.isnull().sum()\n",
    "features_with_missing = missing_summary[missing_summary > 0]\n",
    "\n",
    "if len(features_with_missing) > 0:\n",
    "    print(f\"   Features with missing values:\")\n",
    "    for feat, count in features_with_missing.items():\n",
    "        print(f\"     {feat}: {count} ({count/len(X_clean):.1f}%)\")\n",
    "    \n",
    "    # Fill missing values column by column to avoid shape issues\n",
    "    for col in X_clean.columns:\n",
    "        if X_clean[col].isnull().sum() > 0:\n",
    "            median_val = X_clean[col].median()\n",
    "            X_clean[col] = X_clean[col].fillna(median_val)\n",
    "            print(f\"     ‚Üí Filled {col} missing values with median: {median_val:.2f}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Applied median imputation\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ No missing values found\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "remaining_missing = X_clean.isnull().sum().sum()\n",
    "print(f\"   Remaining missing values: {remaining_missing}\")\n",
    "\n",
    "print(f\"\\nüìã FINAL FEATURE MATRIX:\")\n",
    "print(f\"   Shape: {X_clean.shape}\")\n",
    "print(f\"   Features: {list(X_clean.columns)}\")\n",
    "print(f\"   All features are numerical: {X_clean.dtypes.apply(lambda x: x.kind in 'biufc').all()}\")\n",
    "print(f\"   Target distribution: {y.sum()} positive ({y.mean():.1%}), {(~y).sum()} negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Subject-Level Train/Test Split and Feature Scaling\n",
    "\n",
    "**Current Status - Good Progress:**\n",
    "- ‚úÖ 13 clean base features properly encoded\n",
    "- ‚úÖ Missing values handled (max 2.2%)\n",
    "- ‚úÖ Temporal and categorical features processed\n",
    "\n",
    "**Critical Next Steps:**\n",
    "- Fix target distribution calculation\n",
    "- Create proper subject-level train/test split (prevents data leakage)\n",
    "- Apply feature scaling for ML algorithms\n",
    "- Verify feature quality and correlations\n",
    "\n",
    "**Why Subject-Level Split:**\n",
    "- Prevents data leakage (same subject properties in both train/test)\n",
    "- Simulates real-world deployment scenario\n",
    "- Maintains proper validation methodology\n",
    "\n",
    "**Expected Outcome:**\n",
    "- X_train, X_test, y_train, y_test ready for ML training\n",
    "- Features properly scaled and normalized\n",
    "- No data leakage between splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ CREATING PROPER TRAIN/TEST SPLIT AND SCALING\n",
      "============================================================\n",
      "üìä TARGET DISTRIBUTION CHECK:\n",
      "   Positive samples: 219 (3.0%)\n",
      "   Negative samples: 7027 (97.0%)\n",
      "   Total samples: 7246\n",
      "\n",
      "üîÄ CREATING SUBJECT-LEVEL TRAIN/TEST SPLIT:\n",
      "   Total subjects: 88\n",
      "   Train subjects: 70 (79.5%)\n",
      "   Test subjects: 18 (20.5%)\n",
      "   Train samples: 5980 (170 positive, 2.8%)\n",
      "   Test samples: 1266 (49 positive, 3.9%)\n",
      "\n",
      "‚öñÔ∏è APPLYING FEATURE SCALING:\n",
      "   ‚úÖ Features scaled using StandardScaler\n",
      "   ‚úÖ Scaler fitted on training data only\n",
      "\n",
      "‚úÖ FINAL ML-READY DATASET:\n",
      "   Training set: (5980, 19) features, 5980 samples\n",
      "   Test set: (1266, 19) features, 1266 samples\n",
      "   Feature names: ['gla_sqft', 'bedrooms_total', 'bathrooms_equivalent', 'close_price', 'price_per_sqft', 'latitude', 'longitude', 'distance_km', 'same_city', 'days_from_effective', 'prop_type_clean', 'structure_type_match', 'subject_condition', 'subject_structure_type', 'subject_gla_sqft', 'subject_bedrooms', 'subject_bathrooms', 'subject_municipality_district', 'close_date_numeric']\n",
      "   Class balance - Train: 2.8%, Test: 3.9%\n",
      "\n",
      "üéØ READY FOR ML TRAINING!\n",
      "   ‚úÖ No data leakage (subject-level split)\n",
      "   ‚úÖ Proper feature scaling\n",
      "   ‚úÖ Clean base features only\n",
      "   ‚úÖ 19 features ready for algorithms\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: Create Subject-Level Train/Test Split and Feature Scaling\n",
    "print(\"üéØ CREATING PROPER TRAIN/TEST SPLIT AND SCALING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Fix target distribution calculation\n",
    "print(f\"üìä TARGET DISTRIBUTION CHECK:\")\n",
    "positive_count = y.sum()\n",
    "negative_count = len(y) - positive_count\n",
    "print(f\"   Positive samples: {positive_count} ({positive_count/len(y):.1%})\")\n",
    "print(f\"   Negative samples: {negative_count} ({negative_count/len(y):.1%})\")\n",
    "print(f\"   Total samples: {len(y)}\")\n",
    "\n",
    "# Create subject-level train/test split\n",
    "print(f\"\\nüîÄ CREATING SUBJECT-LEVEL TRAIN/TEST SPLIT:\")\n",
    "unique_subjects = subjects.unique()\n",
    "print(f\"   Total subjects: {len(unique_subjects)}\")\n",
    "\n",
    "# Split subjects 80/20\n",
    "train_subjects, test_subjects = train_test_split(\n",
    "    unique_subjects, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create masks for train/test\n",
    "train_mask = subjects.isin(train_subjects)\n",
    "test_mask = subjects.isin(test_subjects)\n",
    "\n",
    "# Split the data\n",
    "X_train_raw = X_clean[train_mask].copy()\n",
    "X_test_raw = X_clean[test_mask].copy()\n",
    "y_train = y[train_mask].copy()\n",
    "y_test = y[test_mask].copy()\n",
    "\n",
    "print(f\"   Train subjects: {len(train_subjects)} ({len(train_subjects)/len(unique_subjects):.1%})\")\n",
    "print(f\"   Test subjects: {len(test_subjects)} ({len(test_subjects)/len(unique_subjects):.1%})\")\n",
    "print(f\"   Train samples: {len(X_train_raw)} ({y_train.sum()} positive, {y_train.mean():.1%})\")\n",
    "print(f\"   Test samples: {len(X_test_raw)} ({y_test.sum()} positive, {y_test.mean():.1%})\")\n",
    "\n",
    "# Apply feature scaling\n",
    "print(f\"\\n‚öñÔ∏è APPLYING FEATURE SCALING:\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only, transform both\n",
    "X_train = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_raw),\n",
    "    columns=X_train_raw.columns,\n",
    "    index=X_train_raw.index\n",
    ")\n",
    "\n",
    "X_test = pd.DataFrame(\n",
    "    scaler.transform(X_test_raw),\n",
    "    columns=X_test_raw.columns, \n",
    "    index=X_test_raw.index\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Features scaled using StandardScaler\")\n",
    "print(f\"   ‚úÖ Scaler fitted on training data only\")\n",
    "\n",
    "# Verify final dataset\n",
    "print(f\"\\n‚úÖ FINAL ML-READY DATASET:\")\n",
    "print(f\"   Training set: {X_train.shape} features, {len(y_train)} samples\")\n",
    "print(f\"   Test set: {X_test.shape} features, {len(y_test)} samples\")\n",
    "print(f\"   Feature names: {list(X_train.columns)}\")\n",
    "print(f\"   Class balance - Train: {y_train.mean():.1%}, Test: {y_test.mean():.1%}\")\n",
    "\n",
    "print(f\"\\nüéØ READY FOR ML TRAINING!\")\n",
    "print(f\"   ‚úÖ No data leakage (subject-level split)\")\n",
    "print(f\"   ‚úÖ Proper feature scaling\")\n",
    "print(f\"   ‚úÖ Clean base features only\")\n",
    "print(f\"   ‚úÖ {X_train.shape[1]} features ready for algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç SAMPLE ROWS OF NUMERICAL FEATURES\n",
      "============================================================\n",
      "üìä RAW FEATURES (before scaling):\n",
      "First 5 rows of X_train_raw:\n",
      "     gla_sqft  bedrooms_total  bathrooms_equivalent  close_price  \\\n",
      "121    1472.0               3                   2.0     545000.0   \n",
      "122    1350.0               3                   1.0     395000.0   \n",
      "123    1320.0               3                   2.0     378275.0   \n",
      "124    1182.0               3                   2.0     500000.0   \n",
      "125    1136.0               2                   2.0     710000.0   \n",
      "\n",
      "     price_per_sqft  latitude  longitude  distance_km  same_city  \\\n",
      "121      370.244565   44.7865   -63.1475     8.392139          1   \n",
      "122      292.592593   44.7903   -63.1199     6.322417          1   \n",
      "123      286.571970   44.7584   -63.0603     2.460445          1   \n",
      "124      423.011844   44.7832   -63.1657     9.787332          1   \n",
      "125      625.000000   44.7765   -63.0424     0.000000          1   \n",
      "\n",
      "     days_from_effective  prop_type_clean  structure_type_match  \\\n",
      "121                  -15                2                     0   \n",
      "122                  140                2                     0   \n",
      "123                  132                2                     0   \n",
      "124                   70                2                     0   \n",
      "125                  169                2                     0   \n",
      "\n",
      "     subject_condition  subject_structure_type  subject_gla_sqft  \\\n",
      "121                  0                       1            1500.0   \n",
      "122                  0                       1            1500.0   \n",
      "123                  0                       1            1500.0   \n",
      "124                  0                       1            1500.0   \n",
      "125                  0                       1            1500.0   \n",
      "\n",
      "     subject_bedrooms  subject_bathrooms  subject_municipality_district  \\\n",
      "121               3.0                2.5                             52   \n",
      "122               3.0                2.5                             52   \n",
      "123               3.0                2.5                             52   \n",
      "124               3.0                2.5                             52   \n",
      "125               3.0                2.5                             52   \n",
      "\n",
      "     close_date_numeric  \n",
      "121               20210  \n",
      "122               20055  \n",
      "123               20063  \n",
      "124               20125  \n",
      "125               20026  \n",
      "\n",
      "üìä SCALED FEATURES (after scaling):\n",
      "First 5 rows of X_train (scaled):\n",
      "     gla_sqft  bedrooms_total  bathrooms_equivalent  close_price  \\\n",
      "121  0.069071        0.099170              0.343619    -0.341117   \n",
      "122 -0.100544        0.099170             -0.385694    -0.656095   \n",
      "123 -0.142253        0.099170              0.343619    -0.691215   \n",
      "124 -0.334112        0.099170              0.343619    -0.435610   \n",
      "125 -0.398066       -0.726783              0.343619     0.005359   \n",
      "\n",
      "     price_per_sqft  latitude  longitude  distance_km  same_city  \\\n",
      "121       -0.568450 -0.832750   1.615503     2.692792   0.423385   \n",
      "122       -0.841437 -0.831675   1.616919     1.829783   0.423385   \n",
      "123       -0.862602 -0.840704   1.619978     0.219462   0.423385   \n",
      "124       -0.382946 -0.833684   1.614568     3.274544   0.423385   \n",
      "125        0.327147 -0.835581   1.620897    -0.806466   0.423385   \n",
      "\n",
      "     days_from_effective  prop_type_clean  structure_type_match  \\\n",
      "121            -1.166190        -0.203273             -0.361749   \n",
      "122             2.606901        -0.203273             -0.361749   \n",
      "123             2.412160        -0.203273             -0.361749   \n",
      "124             0.902924        -0.203273             -0.361749   \n",
      "125             3.312834        -0.203273             -0.361749   \n",
      "\n",
      "     subject_condition  subject_structure_type  subject_gla_sqft  \\\n",
      "121          -1.079824               -0.788676         -0.233942   \n",
      "122          -1.079824               -0.788676         -0.233942   \n",
      "123          -1.079824               -0.788676         -0.233942   \n",
      "124          -1.079824               -0.788676         -0.233942   \n",
      "125          -1.079824               -0.788676         -0.233942   \n",
      "\n",
      "     subject_bedrooms  subject_bathrooms  subject_municipality_district  \\\n",
      "121         -0.022805           0.642905                       0.975721   \n",
      "122         -0.022805           0.642905                       0.975721   \n",
      "123         -0.022805           0.642905                       0.975721   \n",
      "124         -0.022805           0.642905                       0.975721   \n",
      "125         -0.022805           0.642905                       0.975721   \n",
      "\n",
      "     close_date_numeric  \n",
      "121            1.231971  \n",
      "122           -2.577735  \n",
      "123           -2.381105  \n",
      "124           -0.857223  \n",
      "125           -3.290519  \n",
      "\n",
      "üéØ FEATURE STATISTICS:\n",
      "Feature ranges in raw data:\n",
      "   gla_sqft                      :     250.00 to      6930.00 (mean:  1422.32)\n",
      "   bedrooms_total                :       0.00 to         9.00 (mean:     2.88)\n",
      "   bathrooms_equivalent          :       0.00 to        10.00 (mean:     1.53)\n",
      "   close_price                   :       0.00 to   8775000.00 (mean: 707447.70)\n",
      "   price_per_sqft                :       1.50 to      4584.13 (mean:   531.94)\n",
      "   latitude                      :      42.10 to        53.43 (mean:    47.73)\n",
      "   longitude                     :    -114.26 to       -62.95 (mean:   -94.62)\n",
      "   distance_km                   :       0.00 to        15.68 (mean:     1.93)\n",
      "   same_city                     :       0.00 to         1.00 (mean:     0.85)\n",
      "   days_from_effective           :     -82.00 to       180.00 (mean:    32.91)\n",
      "   prop_type_clean               :       0.00 to         8.00 (mean:     2.48)\n",
      "   structure_type_match          :       0.00 to         1.00 (mean:     0.12)\n",
      "   subject_condition             :       0.00 to         3.00 (mean:     1.61)\n",
      "   subject_structure_type        :       0.00 to         8.00 (mean:     2.71)\n",
      "   subject_gla_sqft              :     522.00 to      3543.10 (mean:  1667.55)\n",
      "   subject_bedrooms              :       1.00 to         8.00 (mean:     3.03)\n",
      "   subject_bathrooms             :       1.00 to         4.00 (mean:     1.94)\n",
      "   subject_municipality_district :       0.00 to        73.00 (mean:    30.91)\n",
      "   close_date_numeric            :   20012.00 to     20221.00 (mean: 20159.88)\n",
      "\n",
      "üéØ SCALED FEATURE STATISTICS:\n",
      "Feature ranges in scaled data (should be ~-3 to +3):\n",
      "   gla_sqft                      :  -1.63 to   7.66 (mean:   0.00)\n",
      "   bedrooms_total                :  -2.38 to   5.05 (mean:   0.00)\n",
      "   bathrooms_equivalent          :  -1.12 to   6.18 (mean:  -0.00)\n",
      "   close_price                   :  -1.49 to  16.94 (mean:   0.00)\n",
      "   price_per_sqft                :  -1.86 to  14.25 (mean:  -0.00)\n",
      "   latitude                      :  -1.59 to   1.61 (mean:  -0.00)\n",
      "   longitude                     :  -1.01 to   1.63 (mean:   0.00)\n",
      "   distance_km                   :  -0.81 to   5.73 (mean:  -0.00)\n",
      "   same_city                     :  -2.36 to   0.42 (mean:  -0.00)\n",
      "   days_from_effective           :  -2.80 to   3.58 (mean:  -0.00)\n",
      "   prop_type_clean               :  -1.05 to   2.32 (mean:   0.00)\n",
      "   structure_type_match          :  -0.36 to   2.76 (mean:   0.00)\n",
      "   subject_condition             :  -1.08 to   0.94 (mean:   0.00)\n",
      "   subject_structure_type        :  -1.25 to   2.45 (mean:  -0.00)\n",
      "   subject_gla_sqft              :  -1.60 to   2.62 (mean:  -0.00)\n",
      "   subject_bedrooms              :  -1.37 to   3.34 (mean:  -0.00)\n",
      "   subject_bathrooms             :  -1.07 to   2.36 (mean:  -0.00)\n",
      "   subject_municipality_district :  -1.43 to   1.95 (mean:  -0.00)\n",
      "   close_date_numeric            :  -3.63 to   1.50 (mean:   0.00)\n",
      "\n",
      "üìã SAMPLE WITH TARGET LABELS:\n",
      "Sample rows with features and target:\n",
      "     gla_sqft  subject_gla_sqft  distance_km  close_price  same_city  \\\n",
      "121    1472.0            1500.0     8.392139     545000.0          1   \n",
      "122    1350.0            1500.0     6.322417     395000.0          1   \n",
      "123    1320.0            1500.0     2.460445     378275.0          1   \n",
      "124    1182.0            1500.0     9.787332     500000.0          1   \n",
      "125    1136.0            1500.0     0.000000     710000.0          1   \n",
      "\n",
      "     is_selected  \n",
      "121            0  \n",
      "122            0  \n",
      "123            0  \n",
      "124            0  \n",
      "125            1  \n"
     ]
    }
   ],
   "source": [
    "# Show sample rows of the numerical features\n",
    "print(\"üîç SAMPLE ROWS OF NUMERICAL FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"üìä RAW FEATURES (before scaling):\")\n",
    "print(\"First 5 rows of X_train_raw:\")\n",
    "print(X_train_raw.head())\n",
    "\n",
    "print(f\"\\nüìä SCALED FEATURES (after scaling):\")\n",
    "print(\"First 5 rows of X_train (scaled):\")\n",
    "print(X_train.head())\n",
    "\n",
    "print(f\"\\nüéØ FEATURE STATISTICS:\")\n",
    "print(\"Feature ranges in raw data:\")\n",
    "for col in X_train_raw.columns:\n",
    "    min_val = X_train_raw[col].min()\n",
    "    max_val = X_train_raw[col].max()\n",
    "    mean_val = X_train_raw[col].mean()\n",
    "    print(f\"   {col:30s}: {min_val:10.2f} to {max_val:12.2f} (mean: {mean_val:8.2f})\")\n",
    "\n",
    "print(f\"\\nüéØ SCALED FEATURE STATISTICS:\")\n",
    "print(\"Feature ranges in scaled data (should be ~-3 to +3):\")\n",
    "for col in X_train.columns:\n",
    "    min_val = X_train[col].min()\n",
    "    max_val = X_train[col].max()\n",
    "    mean_val = X_train[col].mean()\n",
    "    print(f\"   {col:30s}: {min_val:6.2f} to {max_val:6.2f} (mean: {mean_val:6.2f})\")\n",
    "\n",
    "print(f\"\\nüìã SAMPLE WITH TARGET LABELS:\")\n",
    "print(\"Sample rows with features and target:\")\n",
    "sample_indices = X_train.head().index\n",
    "sample_data = pd.DataFrame({\n",
    "    'gla_sqft': X_train_raw.loc[sample_indices, 'gla_sqft'],\n",
    "    'subject_gla_sqft': X_train_raw.loc[sample_indices, 'subject_gla_sqft'], \n",
    "    'distance_km': X_train_raw.loc[sample_indices, 'distance_km'],\n",
    "    'close_price': X_train_raw.loc[sample_indices, 'close_price'],\n",
    "    'same_city': X_train_raw.loc[sample_indices, 'same_city'],\n",
    "    'is_selected': y_train.loc[sample_indices]\n",
    "})\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Train Multiple ML Models and Evaluate Performance\n",
    "\n",
    "**What we're doing:**\n",
    "- Train multiple ML algorithms to learn appraiser selection patterns\n",
    "- Handle class imbalance with appropriate techniques\n",
    "- Evaluate models using precision/recall metrics (not accuracy)\n",
    "- Compare model performance to find the best approach\n",
    "\n",
    "**Models to test:**\n",
    "- **Random Forest**: Excellent for feature interactions and interpretability\n",
    "- **Gradient Boosting (XGBoost)**: State-of-the-art for tabular data\n",
    "- **Logistic Regression**: Simple baseline with good interpretability\n",
    "- **Support Vector Machine**: Good for high-dimensional data\n",
    "\n",
    "**Key considerations:**\n",
    "- **Class imbalance**: Only 3% positive samples, so we'll use class weights\n",
    "- **Metrics**: Focus on Precision, Recall, F1-score, and AUC-ROC\n",
    "- **Feature importance**: Understand what drives appraiser decisions\n",
    "- **Cross-validation**: Ensure robust performance estimates\n",
    "\n",
    "**Expected outcome:**\n",
    "- Trained models that can predict appraiser-quality property selections\n",
    "- Performance comparison to identify the best approach\n",
    "- Feature importance insights into appraiser decision patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ TRAINING MULTIPLE ML MODELS\n",
      "============================================================\n",
      "üìä CLASS IMBALANCE HANDLING:\n",
      "   Class weights: {0: np.float64(0.5146299483648882), 1: np.float64(17.58823529411765)}\n",
      "   Positive class weight: 17.59x\n",
      "\n",
      "üîÑ TRAINING AND EVALUATING MODELS:\n",
      "\n",
      "   Training Random Forest...\n",
      "     ‚úÖ Random Forest trained - AUC: 0.615\n",
      "\n",
      "   Training XGBoost...\n",
      "     ‚úÖ XGBoost trained - AUC: 0.651\n",
      "\n",
      "   Training Logistic Regression...\n",
      "     ‚úÖ Logistic Regression trained - AUC: 0.626\n",
      "\n",
      "   Training SVM...\n",
      "     ‚úÖ SVM trained - AUC: 0.647\n",
      "\n",
      "üìä DETAILED MODEL PERFORMANCE:\n",
      "================================================================================\n",
      "\n",
      "üéØ RANDOM FOREST:\n",
      "   AUC-ROC Score: 0.615\n",
      "   Precision (Class 1): 0.091\n",
      "   Recall (Class 1): 0.184\n",
      "   F1-Score (Class 1): 0.122\n",
      "   True Positives: 9/49 (18.4%)\n",
      "   False Positives: 90\n",
      "\n",
      "üéØ XGBOOST:\n",
      "   AUC-ROC Score: 0.651\n",
      "   Precision (Class 1): 0.135\n",
      "   Recall (Class 1): 0.102\n",
      "   F1-Score (Class 1): 0.116\n",
      "   True Positives: 5/49 (10.2%)\n",
      "   False Positives: 32\n",
      "\n",
      "üéØ LOGISTIC REGRESSION:\n",
      "   AUC-ROC Score: 0.626\n",
      "   Precision (Class 1): 0.050\n",
      "   Recall (Class 1): 0.878\n",
      "   F1-Score (Class 1): 0.095\n",
      "   True Positives: 43/49 (87.8%)\n",
      "   False Positives: 817\n",
      "\n",
      "üéØ SVM:\n",
      "   AUC-ROC Score: 0.647\n",
      "   Precision (Class 1): 0.058\n",
      "   Recall (Class 1): 0.592\n",
      "   F1-Score (Class 1): 0.105\n",
      "   True Positives: 29/49 (59.2%)\n",
      "   False Positives: 474\n",
      "\n",
      "üèÜ BEST MODEL: XGBoost\n",
      "   AUC Score: 0.651\n",
      "\n",
      "üîç TOP 10 FEATURE IMPORTANCES (XGBoost):\n",
      "    1. structure_type_match          : 0.139\n",
      "    2. subject_gla_sqft              : 0.091\n",
      "    3. prop_type_clean               : 0.085\n",
      "    4. subject_municipality_district : 0.077\n",
      "    5. gla_sqft                      : 0.070\n",
      "    6. subject_bedrooms              : 0.062\n",
      "    7. longitude                     : 0.051\n",
      "    8. subject_bathrooms             : 0.045\n",
      "    9. days_from_effective           : 0.043\n",
      "   10. subject_structure_type        : 0.042\n",
      "\n",
      "‚úÖ MODEL TRAINING COMPLETE!\n",
      "   Best performing model: XGBoost\n",
      "   Ready for deployment and further analysis\n"
     ]
    }
   ],
   "source": [
    "# STEP 9: TRAIN MULTIPLE ML MODELS AND EVALUATE PERFORMANCE\n",
    "print(\"ü§ñ TRAINING MULTIPLE ML MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "print(f\"üìä CLASS IMBALANCE HANDLING:\")\n",
    "print(f\"   Class weights: {class_weight_dict}\")\n",
    "print(f\"   Positive class weight: {class_weight_dict[1]:.2f}x\")\n",
    "\n",
    "# Define models with class weights\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        scale_pos_weight=class_weight_dict[1],\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ),\n",
    "    'SVM': SVC(\n",
    "        class_weight='balanced',\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(f\"\\nüîÑ TRAINING AND EVALUATING MODELS:\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n   Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'auc_score': auc_score\n",
    "    }\n",
    "    \n",
    "    print(f\"     ‚úÖ {name} trained - AUC: {auc_score:.3f}\")\n",
    "\n",
    "# Display detailed results\n",
    "print(f\"\\nüìä DETAILED MODEL PERFORMANCE:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"\\nüéØ {name.upper()}:\")\n",
    "    print(f\"   AUC-ROC Score: {result['auc_score']:.3f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(y_test, result['y_pred'], output_dict=True)\n",
    "    print(f\"   Precision (Class 1): {report['1']['precision']:.3f}\")\n",
    "    print(f\"   Recall (Class 1): {report['1']['recall']:.3f}\")\n",
    "    print(f\"   F1-Score (Class 1): {report['1']['f1-score']:.3f}\")\n",
    "    \n",
    "    # Show confusion matrix info\n",
    "    tp = sum((y_test == 1) & (result['y_pred'] == 1))\n",
    "    fp = sum((y_test == 0) & (result['y_pred'] == 1))\n",
    "    fn = sum((y_test == 1) & (result['y_pred'] == 0))\n",
    "    tn = sum((y_test == 0) & (result['y_pred'] == 0))\n",
    "    \n",
    "    print(f\"   True Positives: {tp}/{y_test.sum()} ({tp/y_test.sum():.1%})\")\n",
    "    print(f\"   False Positives: {fp}\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['auc_score'])\n",
    "best_model = results[best_model_name]\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   AUC Score: {best_model['auc_score']:.3f}\")\n",
    "\n",
    "# Feature importance for tree-based models\n",
    "if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "    print(f\"\\nüîç TOP 10 FEATURE IMPORTANCES ({best_model_name}):\")\n",
    "    \n",
    "    if best_model_name == 'Random Forest':\n",
    "        importances = trained_models[best_model_name].feature_importances_\n",
    "    else:  # XGBoost\n",
    "        importances = trained_models[best_model_name].feature_importances_\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):\n",
    "        print(f\"   {i+1:2d}. {row['feature']:30s}: {row['importance']:.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ MODEL TRAINING COMPLETE!\")\n",
    "print(f\"   Best performing model: {best_model_name}\")\n",
    "print(f\"   Ready for deployment and further analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
